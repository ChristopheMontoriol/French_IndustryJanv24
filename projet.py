# -*- coding: utf-8 -*-
"""Projet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15ivoNrZkPtxZWxHEF9nXqbvoJCKxTLvQ

# Import des fichiers csv et des modules ...
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Rajouter les bibliothèques-fonctions utilisées dans ce projet
# %matplotlib inline
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import statsmodels.api as sm

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error


# Pour éviter d'avoir les messages warning
import warnings
warnings.filterwarnings('ignore')

# Installation des modules carte
# Mis en commentaire car long à charger à débloquer !!!!

# !pip install geopandas
# !pip install geopandas geoviews

etablissement=pd.read_csv('/content/drive/MyDrive/Data_Industry/base_etablissement_par_tranche_effectif.csv',sep=',')
geographic=pd.read_csv('/content/drive/MyDrive/Data_Industry/name_geographic_information.csv',sep=',')
population=pd.read_csv('/content/drive/MyDrive/Data_Industry/population.csv',sep=',')
salaire=pd.read_csv('/content/drive/MyDrive/Data_Industry/net_salary_per_town_categories.csv',sep=',')

"""# Pré-processing"""

print(salaire.columns)

# ---------------------------------------------------------------------------------------------------------------
# PREPARATION DU DATAFRAME SALAIRE
# ---------------------------------------------------------------------------------------------------------------
# Modifier les valeurs de la variable CODGEO
# pour la faire coincider avec les valeurs prises par la variable Code_Insee du dataframe geographic
# et ainsi préparer le MERGE

# Dictionnaire de correspondance entre anciens et nouveaux noms de colonne du dataframe salaire
new_column_names_salaire = {
    'SNHM14': 'salaire',
    'SNHMC14': 'salaire_cadre',
    'SNHMP14': 'salaire_cadre_moyen',
    'SNHME14': 'salaire_employe',
    'SNHMO14': 'salaire_travailleur',
    'SNHMF14': 'salaire_femme',
    'SNHMFC14': 'salaire_cadre_femme',
    'SNHMFP14': 'salaire_cadre_moyen_femme',
    'SNHMFE14': 'salaire_employe_femme',
    'SNHMFO14': 'salaire_travailleur_femme',
    'SNHMH14': 'salaire_homme',
    'SNHMHC14': 'salaire_cadre_homme',
    'SNHMHP14': 'salaire_cadre_moyen_homme',
    'SNHMHE14': 'salaire_employe_homme',
    'SNHMHO14': 'salaire_travailleur_homme',
    'SNHM1814': 'salaire_18-25',
    'SNHM2614': 'salaire_26-50',
    'SNHM5014': 'salaire_+50',
    'SNHMF1814': 'salaire_18-25_femme',
    'SNHMF2614': 'salaire_26-50_femme',
    'SNHMF5014': 'salaire_+50_femme',
    'SNHMH1814': 'salaire_18-25_homme',
    'SNHMH2614': 'salaire_26-50_homme',
    'SNHMH5014': 'salaire_+50_homme'
}

# Renommer les colonnes du dataframe salaire
salaire = salaire.rename(columns=new_column_names_salaire)


# Supprimer les zéros en première position
salaire['CODGEO'] = salaire['CODGEO'].str.lstrip('0')

# Remplacer les lettres A ou B par des zéros
salaire['CODGEO'] = salaire['CODGEO'].str.replace('A', '0').str.replace('B', '0')


# Suppression de la colonne LIBGEO qui sera reprise du dataframe geographic
#salaire=salaire.drop(columns=['LIBGEO'])

# ---------------------------------------------------------------------------------------------------------------
# PREPARATION DU DATAFRAME ETABLISSEMENT
# ---------------------------------------------------------------------------------------------------------------
# Modifier les valeurs de la variable CODGEO
# pour la faire coincider avec les valeurs prises par la variable Code_Insee du dataframe geographic
# et ainsi préparer le MERGE


# Dictionnaire de correspondance entre anciens et nouveaux noms de colonne du dataframe etablissement
new_column_names_etab = {
    'E14TST': 'Nbre_etab',
    'E14TS0ND': 'Nbre_etab_0_x',
    'E14TS1': 'Nbre_etab_1-5',
    'E14TS6': 'Nbre_etab_6-9',
    'E14TS10': 'Nbre_etab_10-19',
    'E14TS20': 'Nbre_etab_20-49',
    'E14TS50': 'Nbre_etab_50-99',
    'E14TS100': 'Nbre_etab_100-199',
    'E14TS200': 'Nbre_etab_200-499',
    'E14TS500': 'Nbre_etab_+500',

}

# Renommer les colonnes du dataframe etablissement
etablissement=etablissement.rename(columns=new_column_names_etab)
etablissement['Total'] = etablissement['Nbre_etab_1-5'] + etablissement['Nbre_etab_6-9'] + etablissement['Nbre_etab_10-19'] + etablissement['Nbre_etab_20-49'] + etablissement['Nbre_etab_50-99'] + etablissement['Nbre_etab_100-199'] + etablissement['Nbre_etab_200-499'] + etablissement['Nbre_etab_+500']

etablissement['%Etab1-9'] = (etablissement['Nbre_etab_1-5'] + etablissement['Nbre_etab_6-9']) * 100 / etablissement['Total']
etablissement['%Etab10-49'] = (etablissement['Nbre_etab_10-19'] + etablissement['Nbre_etab_20-49']) * 100 / etablissement['Total']
etablissement['%Etab50-199'] = (etablissement['Nbre_etab_50-99'] + etablissement['Nbre_etab_100-199']) * 100 / etablissement['Total']
etablissement['%Etab200et+'] = (etablissement['Nbre_etab_200-499'] + etablissement['Nbre_etab_+500']) * 100 / etablissement['Total']


etablissement=etablissement.dropna()

# Supprimer les zéros en première position
etablissement['CODGEO'] = etablissement['CODGEO'].str.lstrip('0')

# Remplacer les lettres A ou B par des zéros
etablissement['CODGEO'] = etablissement['CODGEO'].str.replace('A', '0').str.replace('B', '0')


# Suppression des colonnes  LIBGEO,REG et DEP et E14TSxx qui sont aussi dans etablissement
#etablissement=etablissement.drop(columns=['LIBGEO','REG','DEP','E14TST','E14TS0ND','E14TS1','E14TS6','E14TS10','E14TS20','E14TS50','E14TS100','E14TS200','E14TS500'])

# ------------------------------------------------------------------------------------------------------
etablissement=etablissement.drop(columns=['LIBGEO','REG','DEP'])
# ------------------------------------------------------------------------------------------------------

print(etablissement.columns)

# ---------------------------------------------------------------------------------------------------------------
# PREPARATION DU DATAFRAME POPULATION
# -------------------------------------------------------------------------------------------------------------
# Modifier les valeurs de la variable CODGEO
# pour la faire coincider avec les valeurs prises par la variable Code_Insee du dataframe geographic
# et ainsi préparer le MERGE


population=population.drop(columns=['NIVGEO'])
# Convertir la colonne CODGEO en chaînes de caractères
population['CODGEO'] = population['CODGEO'].astype(str)


# Supprimer les zéros en première position
population['CODGEO'] = population['CODGEO'].str.lstrip('0')

# Remplacer les lettres A ou B par des zéros
population['CODGEO'] = population['CODGEO'].str.replace('A', '0').str.replace('B', '0')

print(population.columns)

# ---------------------------------------------------------------------------------------------------------------
# PREPARATION DU DATAFRAME GEOGRAPHIC
# ----------------------------------------------------------------------------------------------------------------

# Renommer la colonne 'code_insee' en 'CODGEO' dans le dataframe 'geographic'
geographic = geographic.rename(columns={'code_insee': 'CODGEO'})

geographic['CODGEO'] = geographic['CODGEO'].astype(str)

# Attention pour un même code_insee = CODGEO de geographic il peut il y avoir plusieurs circonscriptions
#  (Exemple code_insee=75056 Paris il y a 21 circonscriptions)
# il faut avant de faire le MERGE avec etablissement sortir ces doublons.
# Pour cela on ne va MERGER qu'avec les colonnes "utiles" en enlevant la notion de circonscription qui crée ces doublons


colonnes_a_supprimer = ['chef.lieu_région', 'préfecture', 'numéro_circonscription', 'codes_postaux', 'latitude', 'longitude', 'éloignement']

# Supprimer les colonnes spécifiées
geographic = geographic.drop(colonnes_a_supprimer, axis=1)


# Suppression des doublons pour ne garder qu'un seul CODGEO dans le dataframe geographic pour les différentes circonscriptions
geographic = geographic.drop_duplicates()



# De plus 21 CODGEO du dataframe etablissement ne sont pas présents dans le dataframe geographic
# Dans les 21 codes les codes 976 ne sont pas gardés car non présents dans le dataframe salaire
# Les autres ont moins de 100 entreprises donc on ne les garde pas non plus
# Seul le code 61483-Bagnoles de l'Orne code_insee 61022 contient 214 entreprises

# Modifier la valeur de CODGEO de "61022" à "61483"
geographic.loc[geographic['CODGEO'] == '61022', 'CODGEO'] = '61483'

# Attention xxxxx- la commune est bien dans geographic mais pas avec le même code OU communes présentes dans plusieurs régions
# xxxxx la commune n'est pas présente dans géographic!

#----------- moins de 100 etablissements donc le MERGE va supprimer cette info PAS GRAVE
#----------- 26020-La Répara-Auriples (code_insee=26383 REG=82 ET DEP=26 surement le même?) 3 Etablissements!!!!!!
#----------- 31300 Lieoux 2 etablissements
#----------- 35317-Saint-Symphorien (Plusieurs code_insee dans géo 18236/27606/33484/48184/72321/79720 aucun dans le dep 35 et region 53 )
#----------- 52033 Avrecourt
#----------- 52124
#----------- 52266
#----------- 52278
#----------- 52465-
#----------- 55039
#----------- 55050
#----------- 55139
#----------- 55189
#----------- 55239
#----------- 55307
#----------- 62847
#----------- 88106-Ban-sur-Meurthe-Clefcy code_insee 88034 (Seulement 43 entreprises)
#----------- 89326-?


# ---------------------------------------------------------------
# Supprimer dans geographicles DEP= 975 et 976 car nous n'avons aucune information sur les salaires de ces 2 départements
geographic = geographic[~geographic['numéro_département'].isin(['975', '976'])]


# Vérifier s'il y a des doublons dans la colonne CODGEO
doublons_CODGEO = geographic['CODGEO'].duplicated()

# Afficher le nombre de doublons s'il y en a
print('Le nombre de doublons CODGEO est égale à :', doublons_CODGEO.sum())

print(geographic.columns)

geographic.head()

"""# Analyse du dataframe etablissement

"""

etablissement.head()

#La colonne 'REG' a été déjà supprimée.

# Groupement par département et somme du nombre total d'établissements
#total_etab_par_departement = etablissement.groupby('REG')['Nbre_etab_+500'].sum().reset_index()

# Affichage du résultat
#print(total_etab_par_departement)

print("Dimensions du dataframe etablissement:",etablissement.shape,"\n")

etablissement.info()

print("Nombre de données manquantes par variable:\n",etablissement.isna().sum())
print("\n")
print("Nombre de doublons dans le dataframe:\n",etablissement.duplicated().sum())

etablissement.describe()

#La colonne 'REG' a été déjà supprimée.

# Pour ce qui est des types des différentes colonnes :
# Les colonnes CODGEO .. DEP de type str Variables catégorielles pas de calculs sur ces colonnes
# Les colonnes E14xxx de type numérique


#etablissement['REG']=etablissement['REG'].astype(str)

"""Camenbert : Taille des différentes entreprises en France"""

print(etablissement.columns)

# On va vérifier que le nombre d'entreprises comptées par ville est correct pour cela il suffit de vérifier que:
# nbre total d’entreprises dans la ville = Nbre d’entreprises dont on ne connait pas la taille
# + Nbre des entreprises dont on connait les différentes tailles allant de 1 à plus de 500 Employés.
# Cad que pour chaque ligne du dataframe on ait valeur_absolue(E14TST - (E14SOND+E14TS1 + ….+E14TS500)) = 0
# Prendre la valeur absolue va permettre de faire un test en sommant chacune des valeurs et ainsi éviter que des erreurs positives et négatives sur chaque ville ne se compensent

# On définit les colonnes nécessaires pour le calcul
colonnes_somme = ['Nbre_etab_0_x','Nbre_etab_1-5','Nbre_etab_6-9','Nbre_etab_10-19','Nbre_etab_20-49','Nbre_etab_50-99','Nbre_etab_100-199','Nbre_etab_200-499','Nbre_etab_+500']

# Calculer la valeur absolue de chaque ligne selon la formule
verif_comptage_etablissement = etablissement.apply(lambda row: abs(row['Nbre_etab'] - row[colonnes_somme].sum()), axis=1)

# Si la somme de chaque ligne fait 0 alors on est sûr que chaque ligne fait bien 0 puis
if verif_comptage_etablissement.sum()==0:
    print("Pas d'incohérence dans la taille des différentes entreprises du dataframe par localisation")
else:
    print("Attention voir la localisation où se trouve l'incohérence")

# etablissement['CODGEO']=etablissement['CODGEO'].astype(str)
# etablissement[etablissement['CODGEO']=="01024"]

print("Nombre de données uniques dans la dataframe etablissement par variable :\n",etablissement.nunique())

# Exclure la colonne 'E14TST' lors de la sélection des colonnes commençant par 'E14'
e14_columns = [col for col in etablissement.columns if col.startswith('E14') and col not in ['E14TST']]
# Calculer la somme des colonnes sélectionnées
sums = etablissement[e14_columns].sum()
print(sums)


# Afficher les valeurs de la somme dans un diagramme circulaire (camembert)
# Définir l'explosion des parts pour éviter le chevauchement
# plt.figure(figsize=(10,10))
# plt.pie(sums, labels=sums.index, autopct='%1.1f%%',pctdistance=0.8,labeldistance=1.2,explode=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,0.1],shadow=True)
# plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
# plt.title('Répartition des Etablissements en fonction de leurs tailles')
# plt.show()


#fig=px.pie(sums,values=sums,names=sums.index,hole=0.3)
#fig.update_traces(pull=[0.1, 0.1, 0.1, 0.1,0.1, 0.1, 0.1, 0.1,0.1])
#fig.update_layout(title='Répartition des Entreprises par taille',
 #                 width=1000,  # Ajustement de la largeur de la figure
  #                height=800)  # Ajustement de la hauteur de la figure
#fig.show()


# Préparation des données
data = {
    '0': etablissement['Nbre_etab_0_x'].sum(),
    '1-5': etablissement['Nbre_etab_1-5'].sum(),
    '6-9': etablissement['Nbre_etab_6-9'].sum(),
    '10-19': etablissement['Nbre_etab_10-19'].sum(),
    '20-49': etablissement['Nbre_etab_20-49'].sum(),
    '50-99': etablissement['Nbre_etab_50-99'].sum(),
    '100-199': etablissement['Nbre_etab_100-199'].sum(),
    '200-499': etablissement['Nbre_etab_200-499'].sum(),
    '+500': etablissement['Nbre_etab_+500'].sum()
}

# Convertir les données en DataFrame pour faciliter la manipulation avec Plotly
df = pd.DataFrame(list(data.items()), columns=['Taille', 'Nombre'])

# Création du diagramme circulaire
fig = px.pie(df, values='Nombre', names='Taille', title='Répartition des Entreprises par taille',
             hole=0.3)  # Un "trou" au centre pour un style donut
fig.update_traces(textposition='inside', textinfo='percent+label')
fig.update_layout(legend_title_text='Taille des entreprises')
fig.show()

# Trouver les valeurs maximales de chaque variable dans le DataFrame etablissement
valeurs_max = etablissement.max()

# Afficher les valeurs maximales
print("Les valeurs maximales de chaque variable dans le DataFrame etablissement sont :")
print(valeurs_max)

# 1. Valeurs de CODGEO présentes dans les deux DataFrames
valeurs_communes = pd.merge(salaire['CODGEO'], etablissement['CODGEO'], how='inner', on='CODGEO')
nb_valeurs_communes = len(valeurs_communes)
print("Nombre de valeurs de CODGEO présentes dans les deux DataFrames :", nb_valeurs_communes)

# 2. Valeurs de CODGEO présentes dans salaire mais pas dans etablissement
valeurs_uniques_salaire = salaire[~salaire['CODGEO'].isin(etablissement['CODGEO'])]['CODGEO']
nb_valeurs_uniques_salaire = len(valeurs_uniques_salaire)
print("Nombre de valeurs de CODGEO présentes dans salaire mais pas dans etablissement :", nb_valeurs_uniques_salaire)


# 3. Valeurs de CODGEO présentes dans etablissement mais pas dans salaire
valeurs_uniques_etablissement = etablissement[~etablissement['CODGEO'].isin(salaire['CODGEO'])]['CODGEO']
nb_valeurs_uniques_etablissement = len(valeurs_uniques_etablissement)
print("Nombre de valeurs de CODGEO présentes dans etablissement mais pas dans salaire :", nb_valeurs_uniques_etablissement)

# Affichage des CODGEO présents dans établissement mais pas dans salaire

print("\nValeurs de CODGEO présentes dans etablissement mais pas dans salaire :")
print(valeurs_uniques_etablissement)

# # Merge entre le dataframe etablissement et salaire clé = CODGEO

salaire_etablissement = pd.merge(salaire, etablissement, on='CODGEO', how='inner')

# Affichage du DataFrame fusionné
salaire_etablissement.head()

# Création du scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(salaire_etablissement['Nbre_etab_+500'], salaire_etablissement['salaire'], alpha=0.5)
plt.title('Relation entre le salaire et Etablissement')
plt.xlabel('Nbre_etab_+500')
plt.ylabel('Salaire')
plt.grid(True)
plt.show()

"""# Analyse Statistique établissement

Affichage des Q-Q Plot pour chaque colonne numérique du dataframe établissement
"""

import pylab
import scipy.stats as stats
from scipy.stats import shapiro
from ipywidgets import interact, IntSlider, FloatSlider, Dropdown, Button, HBox, VBox
import ipywidgets as widgets

etablissement.shape

# Création d'une copie du DataFrame etablissement
etablissement_copy = etablissement.copy()

print(etablissement_copy.columns)

import math

# Sélectionner uniquement les colonnes numériques du DataFrame
colonnes_numeriques = etablissement_copy.select_dtypes(include=[np.number])

# Nombre de colonnes par ligne
colonnes_par_ligne = 5

# Nombre total de colonnes
nombre_de_colonnes = colonnes_numeriques.shape[1]

# Nombre total de lignes nécessaire
nombre_de_lignes = math.ceil(nombre_de_colonnes / colonnes_par_ligne)

# Créer une figure et des axes pour chaque graphique
fig, axes = plt.subplots(nombre_de_lignes, colonnes_par_ligne, figsize=(15, 4 * nombre_de_lignes))

# Parcourir chaque colonne numérique et afficher le Q-Q Plot
for i, colonne in enumerate(colonnes_numeriques.columns):
    # Calculer l'indice de la ligne et de la colonne
    indice_ligne = i // colonnes_par_ligne
    indice_colonne = i % colonnes_par_ligne

    # Créer le Q-Q Plot sur les axes correspondants
    sm.qqplot(etablissement_copy[colonne], line='45', ax=axes[indice_ligne, indice_colonne])
    axes[indice_ligne, indice_colonne].set_title(f'Q-Q Plot {colonne}')

# Ajuster l'espacement entre les sous-graphiques
plt.tight_layout()
plt.show()

"""Nous constatons que les données ne suivent pas une distribution normale. Cela veut dire qu'il existe des variations importantes dans la répartition des entreprises par taille dans différentes régions ou départements. Par exemple, certaines régions pourraient avoir une concentration plus élevée d'entreprises de grande taille, tandis que d'autres pourraient avoir une prédominance d'entreprises de taille plus modeste."""

# normality test
stat, p = shapiro(etablissement_copy['Nbre_etab'])
print('Statistics=%.3f, p=%.5f' % (stat, p))
alpha = 0.05
if p > alpha:
    print('Les données semblent suivre une distribution normale, avec comme Statistics=%.3f, p_value=%.5f' % (stat, p))
else:
    print('Les données ne suivent probablement pas une distribution normale, avec comme Statistics=%.3f, p_value=%.5f' % (stat, p))

"""Statistique de test (Statistics) = 0.014: Cette valeur est très proche de zéro, ce qui indique un écart significatif par rapport à la distribution normale.
P-value = 0.00000: La p-value est inférieure à la seuil alpha de 0.05, ce qui signifie que nous rejetons l'hypothèse nulle selon laquelle les données suivent une distribution normale. Cela confirme que les données sont significativement différentes d'une distribution normale.
"""

# normality test
stat, p = shapiro(etablissement_copy['Nbre_etab_+500'])
print('Statistics=%.3f, p=%.5f' % (stat, p))
alpha = 0.05
if p > alpha:
    print('Les données semblent suivre une distribution normale, avec comme Statistics=%.3f, p_value=%.5f' % (stat, p))
else:
    print('Les données ne suivent probablement pas une distribution normale, avec comme Statistics=%.3f, p_value=%.5f' % (stat, p))

"""Statistique de test (Statistics) = 0.017: Similairement à Nbre_etab, cette valeur très basse suggère un fort écart par rapport à la normale.
P-value = 0.00000: Tout comme le Nbre_etab, la p-value extrêmement basse nous amène à rejeter l'hypothèse nulle pour cette variable également, concluant qu'elle ne suit pas non plus une distribution normale.

# Analyse du dataframe geographic
"""

geographic.head()

geographic.info()

geographic.shape

print("Nombre de données manquantes par variable:\n",geographic.isna().sum())
print("\n")
print("Nombre de doublons dans le dataframe geographic :\n",geographic.duplicated().sum())

geographic.describe(include='all')

print("Nombre de données uniques dans la dataframe geographic par variable :\n",geographic.nunique())

# choisir une grande ville pour l'étude dans la liste chef.lieu_région
#geographic['chef.lieu_région'].unique(). Cette colonne a été déjà supprimée.

geographic['code_région'].unique()

geographic['EU_circo'].unique()

# Verification de la correspondance entre code_insee et CODGEO de etablissement
# Il a fallu faire population['CODGEO']=population['CODGEO'].astype(str) avant pour que cela fonctionne
# code_insee="1024" correspond à Attignat code_postal 01430 dept 01 et region 82
# Attention ici sur 5 caractère ! il faut rajouter le 0

"""```
# Ce texte est au format code
```

# Analyse du dataframe salaire
"""

salaire.head()

print("Dimensions du dataframe salaire :",salaire.shape,"\n")

salaire.info()

print("Nombre de données manquantes par variable:\n",salaire.isna().sum())
print("\n")
print("Nombre de doublons dans le dataframe salaire :\n",salaire.duplicated().sum())

len(salaire['CODGEO'].unique())

salaire.describe()

salaire.shape

"""**Top des régions qui paient moins bien :**"""

# La moyenne des salaires par région
#moyenne_salaire_region = salaire.drop(columns=['CODGEO']).groupby('LIBGEO').mean()

# Trier les régions par salaire moyen croissant
#tri_region_salaire_moyen = moyenne_salaire_region.sort_values(by='salaire')

# Afficher les 5 premières régions avec les salaires moyens les plus bas
#top_regions_salaire_bas = tri_region_salaire_moyen.head()
#print(top_regions_salaire_bas)

"""**Les disparités entre hommes et femmes en fonction des catégories socioprofessionnelles :**"""

# Calculer la disparité relative entre hommes et femmes pour chaque catégorie socio-professionnelle :

# On va mesurer de combien le salaire moyen des hommes est supérieur ou inférieur au salaire moyen des femmes, en pourcentage par rapport au salaire moyen des femmes.
#Ensuite, on va calculer la moyenne de la disparité relative spécifiquement pour chaque catégorie :
#1- disparité salaire moyen :
#disparite_homme_femme = (salaire net moyen - salaire net moyen pour les femmes)/salaire net moyen pour les femmes
disparite_homme_femme = (salaire['salaire'] - salaire['salaire_femme']) / salaire['salaire_femme']
print("Disparité globale : ", disparite_homme_femme.mean()*100,'%')

#2- disparité salaire cadres :
 # disparite_homme_femme_cadres = (salaire net moyen par heure pour les cadres - salaire net moyen par heure pour les cadres féminins)/salaire net moyen par heure pour les cadres féminins
disparite_homme_femme_cadres = (salaire['salaire_cadre'] - salaire['salaire_cadre_femme']) / salaire['salaire_cadre_femme']
print("Disparité pour les cadres : ", disparite_homme_femme_cadres.mean()*100,'%')

#3- disparité salaire cadres moyens:
disparite_homme_femme_cadres_moyens = (salaire['salaire_cadre_moyen'] - salaire['salaire_cadre_moyen_femme']) / salaire['salaire_cadre_moyen_femme']
print("Disparité pour les cadres moyens : ", disparite_homme_femme_cadres_moyens.mean()*100,'%')

#4- disparité salaire employes:
disparite_homme_femme_employes = (salaire['salaire_employe'] - salaire['salaire_employe_femme']) / salaire['salaire_employe_femme']
print("Disparité pour les employés : ", disparite_homme_femme_employes.mean()*100,'%')


#5- disparité salaire travailleurs:
disparite_homme_femme_travailleurs = (salaire['salaire_travailleur'] - salaire['salaire_travailleur_femme']) / salaire['salaire_travailleur_femme']
print("Disparité pour les travailleurs : ", disparite_homme_femme_travailleurs.mean()*100,'%')

"""*En moyenne, le salaire des hommes est supérieur à celui des femmes.
Avec les plus grandes disparités observées chez les cadres et les travailleurs, tandis que les employés présentent la plus faible disparité.*
"""

# Catégories professionnelles
categories = ['Cadres', 'Cadres moyens', 'Employés', 'Travailleurs']

# Valeurs de disparité salariale
disparites = [17.60531468314386, 9.887706605652797, 2.472865187964315, 14.680015141858643]

# Création du graphique à barres
plt.figure(figsize=(10, 6))
plt.bar(categories, disparites, color='skyblue')

# Ajout de titres et de libellés
plt.title('Disparité salariale par catégorie professionnelle')
plt.xlabel('Catégorie professionnelle')
plt.ylabel('Disparité salariale (%)')

# Affichage du graphique
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""**Les disparités entre les hommes et femmes en fonction des tranches d'âge :**


"""

# Calculer la disparité entre hommes et femmes pour chaque tranche âge :

#1- entre 18 et 25 ans
#disparite_homme_femme_18_25 = (salaire net moyen par heure pour les 18-25 ans - salaire net moyen par heure pour les femmes âgées de 18 à 25 ans)/salaire net moyen par heure pour les femmes âgées de 18 à 25 ans
disparite_homme_femme_18_25 = (salaire['salaire_18-25'] - salaire['salaire_18-25_femme']) / salaire['salaire_18-25_femme']
print("Disparité pour les 18-25 ans : ", disparite_homme_femme_18_25.mean()*100,'%')

#2- entre 26 et 50 ans
disparite_homme_femme_26_50 = (salaire['salaire_26-50'] - salaire['salaire_26-50_femme']) / salaire['salaire_26-50_femme']
print("Disparité pour les 26-50 ans : ", disparite_homme_femme_26_50.mean()*100,'%')

#3- plus de 50 ans
disparite_homme_femme_plus_50 = (salaire['salaire_+50'] - salaire['salaire_+50_femme']) / salaire['salaire_+50_femme']
print("Disparité pour les plus de 50 ans : ", disparite_homme_femme_plus_50.mean()*100,'%')

"""*Ces résultats mettent en lumière une tendance à une disparité salariale croissante avec l'âge, les écarts les plus importants étant observés chez les salariés plus âgés. Cela pourrait refléter des inégalités persistantes dans les opportunités professionnelles et les salaires entre hommes et femmes tout au long de leur carrière, avec une accentuation de ces disparités chez les salariés plus expérimentés.*"""

# Tranches d'âge
tranches_age = ['18-25 ans', '26-50 ans', 'Plus de 50 ans']

# Valeurs de disparité salariale
disparites_age = [4.286591078294969, 11.745237278240928, 20.02852196164705]

# Création du graphique à barres
plt.figure(figsize=(10, 6))
plt.bar(tranches_age, disparites_age, color='lightgreen')

# Ajout de titres et de libellés
plt.title('Disparité salariale par tranche d\'âge')
plt.xlabel('Tranche d\'âge')
plt.ylabel('Disparité salariale (%)')

# Affichage du graphique
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

plt.show()

# Création et trie des deux dataframes dans l'ordre croissant en fonction du salaire moyen homme ou femme
df_hcroissant = salaire.sort_values(by = "salaire_homme",ascending = False)
df_fcroissant = salaire.sort_values(by = "salaire_femme",ascending = False)

# Mise en forme de la figure
fig = plt.figure(figsize = (25,10))

# Affichage de la figure
plt.subplot(121)
sns.barplot(x="CODGEO",y="salaire_homme",width = 0.4, color = "b",data = df_hcroissant.head(10)).set(title='Top 10 salaire moyen homme / ville')
plt.xticks(rotation=45)

plt.subplot(122)
sns.barplot(x="CODGEO",y="salaire_femme",width = 0.4, color = "r", data = df_fcroissant.head(10)).set(title='Top 10 salaire moyen femme / ville')
plt.xticks(rotation=45)

plt.show()

"""Cette section permet de regrouper les graphiques liés aux différents dataframes vus précédemments


---

### **Boîte à moustaches pour chaque catégorie socioprofessionnelle : Hommes et femmes** (Issam)
"""

# Sélectionner les colonnes pertinentes
salaires_hommes = salaire[['salaire_cadre_homme', 'salaire_cadre_moyen_homme', 'salaire_employe_homme', 'salaire_travailleur_homme']]
salaires_femmes = salaire[['salaire_cadre_femme', 'salaire_cadre_moyen_femme', 'salaire_employe_femme', 'salaire_travailleur_femme']]

# Créer une boîte à moustaches pour chaque catégorie socioprofessionnelle
plt.figure(figsize=(10, 6))

# Boîte à moustaches pour les salaires des hommes
plt.boxplot([salaires_hommes[col] for col in salaires_hommes.columns], positions=[1, 2, 3, 4], widths=0.4, labels=salaires_hommes.columns, boxprops=dict(color="blue"))

# Boîte à moustaches pour les salaires des femmes
plt.boxplot([salaires_femmes[col] for col in salaires_femmes.columns], positions=[1.4, 2.4, 3.4, 4.4], widths=0.4, labels=salaires_hommes.columns, boxprops=dict(color="red"))

plt.title('Comparaison des salaires entre hommes et femmes pour chaque catégorie socioprofessionnelle')
plt.xlabel('Catégorie socioprofessionnelle')
plt.ylabel('Salaire')
plt.xticks([1.2, 2.2, 3.2, 4.2], ['Cadre', 'Cadre moyen', 'Employé', 'Travailleur'])
plt.legend(['Hommes', 'Femmes'])
plt.grid(True)
plt.tight_layout()
plt.show()

"""D'après ce graphique, il semble y avoir des différences dans la répartition des salaires entre les hommes et les femmes dans chaque catégorie socio-professionnelle. On peut observer que:

- Cadre : Les salaires des hommes cadres montrent une plus grande dispersion comparée à ceux des femmes cadres. Cela suggère une plus grande hétérogénéité dans les salaires des hommes dans cette catégorie. De plus, la médiane pour les hommes est supérieure à celle des femmes, indiquant que la médiane des salaires est plus élevée pour les hommes cadres.

- Cadre moyen : La distribution des salaires des hommes et des femmes dans cette catégorie est presque similaire, bien que la médiane des salaires soit plus élevée pour les hommes. Les plages de salaire (illustrées par la hauteur des boîtes) sont assez similaires, mais les hommes ont quelques valeurs aberrantes supérieures.

- Employé : Les médianes des hommes et des femmes dans cette catégorie sont presque identiques, suggérant peu de différence dans le salaire médian entre les sexes. Cependant, les hommes montrent une distribution de salaires légèrement plus large, avec plus de valeurs aberrantes supérieures et inférieures.

- Travailleur : Cette catégorie montre une médiane des salaires très similaire entre les hommes et les femmes, mais avec une dispersion plus importante parmi les hommes, comme l'indique la plus grande étendue des moustaches et des valeurs aberrantes.

Globalement, le graphique suggère l'existence d'un écart salarial entre les hommes et les femmes, avec des médianes de salaire généralement plus élevées pour les hommes dans toutes les catégories professionnelles examinées. De plus, les hommes ont tendance à avoir une plus grande variabilité dans leurs salaires, particulièrement en tant que cadres et travailleurs. Cela pourrait indiquer des écarts de salaires plus importants au sein de ces catégories pour les hommes que pour les femmes.

Cependant, je tiens à préciser qu'il faut considérer le contexte et les facteurs sous-jacents qui pourraient expliquer ces différences, tels que l'ancienneté, le profil, l'éducation, les heures de travail, et autres facteurs qui ne sont pas précisés dans les resources dont nous disposons.

### **Boîte à moustaches pour chaque tranche d'âge : Hommes et femmes** (Issam)
"""

# Sélectionner les colonnes pertinentes
salaires_hommes = salaire[['salaire_18-25_homme', 'salaire_26-50_homme', 'salaire_+50_homme']]
salaires_femmes = salaire[['salaire_18-25_femme', 'salaire_26-50_femme', 'salaire_+50_femme']]

# Créer une boîte à moustaches pour chaque tranche d'âge :
plt.figure(figsize=(10, 6))

# Boîte à moustaches pour les salaires des hommes
plt.boxplot([salaires_hommes[col] for col in salaires_hommes.columns], positions=[1, 2, 3], widths=0.4, labels=salaires_hommes.columns, boxprops=dict(color="blue"))

# Boîte à moustaches pour les salaires des femmes
plt.boxplot([salaires_femmes[col] for col in salaires_femmes.columns], positions=[1.4, 2.4, 3.4], widths=0.4, labels=salaires_hommes.columns, boxprops=dict(color="red"))

plt.title("Comparaison des salaires entre hommes et femmes pour chaque tranche d'âge")
plt.xlabel("Tranche d'âge")
plt.ylabel('Salaire')
plt.xticks([1.2, 2.2, 3.2], ['salaire_18-25', 'salaire_26-50', 'salaire_+50_homme'])
plt.legend(['Hommes', 'Femmes'])
plt.grid(True)
plt.tight_layout()
plt.show()

"""Le graphique représente une comparaison des salaires entre hommes et femmes sur trois tranches d'âge différentes : 18-25 ans, 26-50 ans et plus de 50 ans. Voici une analyse des résultats :

18-25 ans : Les médianes des salaires pour les hommes et les femmes dans cette tranche d'âge semblent très proches, ce qui indique qu'il y a peu ou pas de différence significative entre les salaires médians des deux sexes. La distribution des salaires est similaire, avec peu de valeurs aberrantes.

26-50 ans : Il y a une différence plus marquée entre les salaires des hommes et des femmes. La médiane pour les hommes est plus élevée que celle des femmes, suggérant que les hommes dans cette tranche d'âge gagnent en médiane plus que les femmes. Les deux distributions présentent des valeurs aberrantes, mais celles des hommes sont plus éloignées, ce qui peut indiquer une plus grande disparité de salaires chez les hommes.

Plus de 50 ans : La médiane des salaires des hommes est également plus élevée dans cette tranche d'âge. La distribution des salaires chez les femmes est plus concentrée, avec moins de valeurs aberrantes que chez les hommes, qui présentent une distribution plus large et des valeurs aberrantes plus éloignées.


En conclusion, les hommes ont tendance à avoir une médiane de salaire plus élevée dans les tranches d'âge de 26 ans et plus, avec également une plus grande variabilité et des valeurs aberrantes plus élevées. Pour les plus jeunes (18-25 ans), les salaires semblent être plus équilibrés entre les sexes.

Ces tendances peuvent être le reflet de différents facteurs, tel que l'expérience de travail, le niveau d'éducation, les différences de carrière ou de secteur d'activité, et d'éventuelles disparités de genre sur le marché du travail.

Tout d'abord, on constate que le top 10 des salaires moyens des femmes est 2 fois moins important comparé à celui des hommes.

Ensuite,  on remarque que les villes représentées dans chacun des top 10 homme et femme ne sont pas exactement les mêmes.

Ainsi, le "sexe" et la "ville" d'un individu devraient jouer un rôle sur sa rémuneration moyenne.

# Analyse Statistique Salaire

Affichage des Q-Q Plot pour chaque colonne numérique du dataframe Salaire
"""

import pylab
import scipy.stats as stats
from scipy.stats import shapiro
from ipywidgets import interact, IntSlider, FloatSlider, Dropdown, Button, HBox, VBox
import ipywidgets as widgets

var_num = salaire.select_dtypes(include=['int', 'float'])
num_cols = 5
num_rows = -(-len(var_num.columns) // num_cols)

fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5*num_rows))

for i, column in enumerate(var_num.columns):
    ax = axes[i // num_cols, i % num_cols]
    sm.qqplot(var_num[column], line='45', fit=True, ax=ax)
    ax.set_title(f"QQ-plot '{column}'")

for i in range(len(var_num.columns), num_rows * num_cols):
    fig.delaxes(axes[i // num_cols, i % num_cols])

plt.tight_layout()
plt.show()

#var_num=salaire.select_dtypes(include=['int','float'])
#for column in var_num.columns:
#  sm.qqplot(var_num[column],line='45',fit=True);
  # Ajouter un titre avec le nom de la colonne
  #plt.title(f"Graphique QQ-plot pour la colonne '{column}'")
  #plt.show()

"""Seules les variables suivantes se rapprochent de la première bissectrice :
salaire_cadre_femme
salaire_18-25_femme
ce qui tend à montrer que ces 2 variables suivent une loi normale.
Nous allons afficher leurs histogrammes pour le confirmer. Voir cellules ci dessous.
Attention moins vrai sur la partie droite du graphique
"""

sns.histplot(salaire['salaire_18-25_femme']);

sns.histplot(salaire['salaire_cadre_femme']);

# normality test
stat, p = shapiro(salaire['salaire_cadre_femme'])
print('Statistics=%.3f, p=%.5f' % (stat, p))
alpha = 0.05
if p > alpha:
    print('Les données semblent suivre une distribution normale, avec comme Statistics=%.3f, p_value=%.5f' % (stat, p))
else:
    print('Les données ne suivent probablement pas une distribution normale, avec comme Statistics=%.3f, p_value=%.5f' % (stat, p))

# normality test
stat, p = shapiro(salaire['salaire_18-25_femme'])
print('Statistics=%.3f, p=%.5f' % (stat, p))
alpha = 0.05
if p > alpha:
    print('Les données semblent suivre une distribution normale, avec comme Statistics=%.3f, p_value=%.5f' % (stat, p))
else:
    print('Les données ne suivent probablement pas une distribution normale, avec comme Statistics=%.3f, p_value=%.5f' % (stat, p))

"""En interprétant ces résultats :

La statistique de test est proche de 1 ce qui indique que les données ne sont pas très éloignées d'une distribution normale, mais ce n'est pas suffisant pour conclure qu'elles la suivent effectivement.
La valeur p_value est égale à zéro ce qui suggère que les données ne suivent probablement pas une distribution normale, car la probabilité d'observer ces données ou des données plus extrêmes est très faible si l'hypothèse de normalité est vraie.
"""

# Matrice de corrélations des variables du dataframe salaire

# salaire.info()

# Exclure les colonnes CODGEO et LIBGEO (Car elles ne contiennent pas de données numériques)
salaire_corr = salaire.drop(columns=['CODGEO', 'LIBGEO'])
#salaire_corr = salaire.drop(columns=['CODGEO']) # il n'y a pas de colonne 'LIBGEO'

#sns.heatmap(salaire_corr.corr())
#plt.title("Matrice de corrélations des variables du dataframe salaire")
#plt.show()

# Avec plotly
matrix_corr=px.imshow(salaire_corr.corr().round(2),text_auto=True)


# Mettre en forme les annotations avec deux chiffres après la virgule
matrix_corr.update_traces(hoverongaps=False)
matrix_corr.update_layout(title='Matrice de corrélation des salaires',
                  xaxis=dict(title='Variables'),
                  yaxis=dict(title='Variables'),
                  width=1000,
                  height=800)





matrix_corr.show()

"""Cette matrice de corrélation montre de façon très significative que les salaires des 18-25 ne sont pas corrélés avec les autres catégories de salaires de cette étude."""

print(salaire.columns)

plt.figure(figsize=(8, 6))
sns.boxplot(salaire['salaire'])
plt.ylabel('salaire')
plt.title('Boîte à moustaches de la variable salaire moyen')
plt.show()



#Afficher les boxplots avec Plotly Express
fig = px.box(salaire_corr, y=salaire_corr.columns)

#Mettre en forme la figure
fig.update_layout(title='Boxplots des Salaires',
                  yaxis=dict(title='Salaire'),
                  width=1000,
                  height=800)

#Afficher la figure
fig.show()

"""Ces boxplot montrent que pour des catégories égales le salaire median et en général même l'écart interquartile (la boite) des hommes se situe toujours au dessus de celui des femmes.
On remarque de plus un faible écart interquartile (Cad la distribution de la variable est concentrée autour de la médiane) pour les variables:
salaire des cadres moyens, employés,travailleurs et pour la tranche d'age 18-25 ans que ce soit chez les hommes ou chez les femmes (Voir calcul des écarts types par variable dans la cellule de code suivante qui montre la même tendance).
Enfin de façon très nette, on remarque que plus on est agé plus le salaire augmente.
"""

salaire_corr.std().sort_values()

"""Analyse des liaisons entre les variables salaire-salaire homme et salaire-salaire femme"""

# H0 : les variables salaire et salaire femme ne sont pas corrélés
# H1 : les 2 varaibles sont corrélées

from scipy.stats import pearsonr
# pearsonr(x=salaire['salaire'],y=salaire['salaire_femme'])

print('p-value femme',pearsonr(x=salaire['salaire'],y=salaire['salaire_femme'])[1])
print('coefficient femme',pearsonr(x=salaire['salaire'],y=salaire['salaire_femme'])[0])

# H0 : les variables salaire et salaire femme ne sont pas corrélés
# H1 : les 2 varaibles sont corrélées

from scipy.stats import pearsonr
# pearsonr(x=salaire['salaire'],y=salaire['salaire_homme'])

print('p-value homme',pearsonr(x=salaire['salaire'],y=salaire['salaire_homme'])[1])
print('coefficient femme',pearsonr(x=salaire['salaire'],y=salaire['salaire_homme'])[0])

"""# Cartes

## Cartes Geoviews
"""

salaire.head()

geographic.head()

df_dep = geographic[['CODGEO','nom_département']]
df_dep.head()

# S'assurer que CODGEO est une chaîne dans les deux DataFrames avant la fusion
salaire['CODGEO'] = salaire['CODGEO'].astype(str).str.lstrip('0').str.replace('A', '').str.replace('B', '')
df_dep['CODGEO'] = df_dep['CODGEO'].astype(str)

# Maintenant, fusionner en utilisant CODGEO comme clé
df_salaire_dep = salaire.merge(right=df_dep, on='CODGEO', how="left")
df_salaire_dep.head()

df_salaire_dep_mean= df_salaire_dep.groupby('nom_département').mean('salaire')
df_salaire_dep_mean.head()

!pip install numpy pandas bokeh holoviews geopandas
!pip install geoviews

import geoviews as gv
import geoviews.feature as gf
import geopandas as gpd
from geoviews import dim
gv.extension('bokeh')

df_geojson = gpd.read_file('/content/drive/MyDrive/Data_Industry/departements-version-simplifiee.geojson')
df_geojson.head()

df_f = df_geojson.merge(df_salaire_dep_mean, left_on='nom', right_on='nom_département')
df_f.head()

"""**Carte des salaires moyens selon le département**

"""

gv.extension('bokeh')
departement = gv.Polygons(df_f, vdims=['nom', 'salaire'])
departement.opts(width=600, height=600, toolbar='above', color=dim('salaire'),colorbar=True, tools=['hover'], aspect='equal')

"""**Carte des salaires moyens de la catégorie 18-25 ans selon le département**

"""

gv.extension('bokeh')
departement_18_25 = gv.Polygons(df_f, vdims=['nom', 'salaire_18-25'])
departement_18_25.opts(width=600, height=600, toolbar='above', color=dim('salaire_18-25'),colorbar=True, tools=['hover'], aspect='equal')

"""**Carte des salaires moyens de la catégorie + 50 ans selon le département**"""

gv.extension('bokeh')
departement_50 = gv.Polygons(df_f, vdims=['nom', 'salaire_+50'])
departement_50.opts(width=600, height=600, toolbar='above', color=dim('salaire_+50'),colorbar=True, tools=['hover'], aspect='equal')

"""**Carte de l'écart-type du salaire moyen par département**"""



#df_salaire_dep_ec= df_salaire_dep.groupby('nom_département').std(ddof=0)
#df_salaire_dep_ec.head()

# Sélectionner uniquement les colonnes numériques
numerical_cols = df_salaire_dep.select_dtypes(include='number').columns
df_salaire_dep_ec = df_salaire_dep.groupby('nom_département')[numerical_cols].std(ddof=0)

print(df_salaire_dep_ec.head())

#regions = gv.Polygons(df_salaire_region_ec, vdims=['nom', ''])
#regions.opts(width=600, height=600, toolbar='above', color=dim(''),colorbar=True, tools=['hover'], aspect='equal')

!pip install basemap-data-hires

!pip install basemap-data-hires

!pip install basemap

etablissement.head()

"""## Carte Basemap

"""

# Géographic2
etablissement2=pd.read_csv('/content/drive/MyDrive/Data_Industry/base_etablissement_par_tranche_effectif.csv',sep=',')
geographic2=pd.read_csv('/content/drive/MyDrive/Data_Industry/name_geographic_information.csv',sep=',')

# PREPARATION DU DATAFRAME ETABLISSEMENT
# ---------------------------------------------------------------------------------------------------------------
# Modifier les valeurs de la variable CODGEO
# pour la faire coincider avec les valeurs prises par la variable Code_Insee du dataframe geographic
# et ainsi préparer le MERGE


# Dictionnaire de correspondance entre anciens et nouveaux noms de colonne du dataframe etablissement
new_column_names_etab = {
    'E14TST': 'Nbre_etab',
    'E14TS0ND': 'Nbre_etab_0_x',
    'E14TS1': 'Nbre_etab_1-5',
    'E14TS6': 'Nbre_etab_6-9',
    'E14TS10': 'Nbre_etab_10-19',
    'E14TS20': 'Nbre_etab_20-49',
    'E14TS50': 'Nbre_etab_50-99',
    'E14TS100': 'Nbre_etab_100-199',
    'E14TS200': 'Nbre_etab_200-499',
    'E14TS500': 'Nbre_etab_+500',

}

# Renommer les colonnes du dataframe etablissement
etablissement2=etablissement2.rename(columns=new_column_names_etab)


# Supprimer les zéros en première position
etablissement2['CODGEO'] = etablissement2['CODGEO'].str.lstrip('0')

# Remplacer les lettres A ou B par des zéros
etablissement2['CODGEO'] = etablissement2['CODGEO'].str.replace('A', '0').str.replace('B', '0')

# ---------------------------------------------------------------------------------------------------------------
# PREPARATION DU DATAFRAME GEOGRAPHIC
# ----------------------------------------------------------------------------------------------------------------

# Renommer la colonne 'code_insee' en 'CODGEO' dans le dataframe 'geographic'
geographic2 = geographic2.rename(columns={'code_insee': 'CODGEO'})

geographic2['CODGEO'] = geographic2['CODGEO'].astype(str)

# Attention pour un même code_insee = CODGEO de geographic il peut il y avoir plusieurs circonscriptions
#  (Exemple code_insee=75056 Paris il y a 21 circonscriptions)
# il faut avant de faire le MERGE avec etablissement sortir ces doublons.
# Pour cela on ne va MERGER qu'avec les colonnes "utiles" en enlevant la notion de circonscription qui crée ces doublons


colonnes_a_supprimer = ['chef.lieu_région', 'préfecture', 'numéro_circonscription', 'codes_postaux']
#colonnes_a_supprimer = ['chef.lieu_région', 'préfecture', 'numéro_circonscription', 'codes_postaux', 'latitude', 'longitude', 'éloignement']

# Supprimer les colonnes spécifiées
geographic2 = geographic2.drop(colonnes_a_supprimer, axis=1)


# Suppression des doublons pour ne garder qu'un seul CODGEO dans le dataframe geographic pour les différentes circonscriptions
geographic2 = geographic2.drop_duplicates()



# De plus 21 CODGEO du dataframe etablissement ne sont pas présents dans le dataframe geographic
# Dans les 21 codes les codes 976 ne sont pas gardés car non présents dans le dataframe salaire
# Les autres ont moins de 100 entreprises donc on ne les garde pas non plus
# Seul le code 61483-Bagnoles de l'Orne code_insee 61022 contient 214 entreprises

# Modifier la valeur de CODGEO de "61022" à "61483"
geographic2.loc[geographic2['CODGEO'] == '61022', 'CODGEO'] = '61483'

# Attention xxxxx- la commune est bien dans geographic mais pas avec le même code OU communes présentes dans plusieurs régions
# xxxxx la commune n'est pas présente dans géographic!

#----------- moins de 100 etablissements donc le MERGE va supprimer cette info PAS GRAVE
#----------- 26020-La Répara-Auriples (code_insee=26383 REG=82 ET DEP=26 surement le même?) 3 Etablissements!!!!!!
#----------- 31300 Lieoux 2 etablissements
#----------- 35317-Saint-Symphorien (Plusieurs code_insee dans géo 18236/27606/33484/48184/72321/79720 aucun dans le dep 35 et region 53 )
#----------- 52033 Avrecourt
#----------- 52124
#----------- 52266
#----------- 52278
#----------- 52465-
#----------- 55039
#----------- 55050
#----------- 55139
#----------- 55189
#----------- 55239
#----------- 55307
#----------- 62847
#----------- 88106-Ban-sur-Meurthe-Clefcy code_insee 88034 (Seulement 43 entreprises)
#----------- 89326-?


# ---------------------------------------------------------------
# Supprimer dans geographicles DEP= 975 et 976 car nous n'avons aucune information sur les salaires de ces 2 départements
geographic2 = geographic2[~geographic2['numéro_département'].isin(['975', '976'])]


# Vérifier s'il y a des doublons dans la colonne CODGEO
doublons_CODGEO = geographic2['CODGEO'].duplicated()

# Afficher le nombre de doublons s'il y en a
print('Le nombre de doublons CODGEO est égale à :', doublons_CODGEO.sum())

# 1
geographic2["longitude"] = geographic2["longitude"].apply(lambda x: str(x).replace(',','.'))
# 2
mask = geographic2["longitude"] == '-'
geographic2.drop(geographic2[mask].index, inplace=True)
# 3
geographic2.dropna(subset = ["longitude", "latitude"], inplace=True)
# 4
geographic2["longitude"] = geographic2["longitude"].astype(float)

geographic2.drop_duplicates(subset=["CODGEO"], keep="first", inplace=True)
paris_lat = geographic2.loc[geographic2["nom_commune"] == "Paris"].iloc[0]["latitude"]
paris_lon = geographic2.loc[geographic2["nom_commune"] == "Paris"].iloc[0]["longitude"]
from math import radians, cos, sin, asin, sqrt

def haversine(lon1, lat1, lon2, lat2):
    # convert decimal degrees to radians
    lon1 = radians(lon1)
    lat1 = radians(lat1)
    lon2 = radians(lon2)
    lat2 = radians(lat2)
    #lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
    # haversine formula
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    # Radius of earth in kilometers is 6371
    km = 6371* c
    return km

distances = []

for index, row in geographic2.iterrows():
    distances.append(haversine(row["longitude"], row["latitude"], paris_lon, paris_lat))

geographic2["distance"] = pd.Series(distances, index=geographic2.index)

industry = etablissement2[etablissement2["CODGEO"].apply(lambda x: str(x).isdigit())]
industry["CODGEO"] = industry["CODGEO"].astype(int)

industry['Micro'] = industry['Nbre_etab_1-5'] + industry['Nbre_etab_6-9']
industry['Small'] = industry['Nbre_etab_10-19'] + industry['Nbre_etab_20-49']
industry['Medium'] = industry['Nbre_etab_50-99'] + industry['Nbre_etab_100-199']
industry['Large_and_Enterprise'] = industry['Nbre_etab_200-499'] + industry['Nbre_etab_+500']

industry['Sum'] = industry['Nbre_etab_1-5'] + industry['Nbre_etab_6-9'] + industry['Nbre_etab_10-19'] + industry['Nbre_etab_20-49'] + industry['Nbre_etab_50-99'] + industry['Nbre_etab_100-199'] + industry['Nbre_etab_200-499'] + industry['Nbre_etab_+500']

industry['Micro%'] = industry['Micro'] * 100 / industry['Sum']
industry['Small%'] = industry['Small'] * 100 / industry['Sum']
industry['Medium%'] = industry['Medium'] * 100 / industry['Sum']
industry['Large_and_Enterprise%'] = industry['Large_and_Enterprise'] * 100 / industry['Sum']

relevant_columns = [
    'CODGEO',
    'LIBGEO', 'REG', 'DEP',
    'Sum', 'Micro', 'Small', 'Medium', 'Large_and_Enterprise',
    'Micro%', 'Small%', 'Medium%', 'Large_and_Enterprise%'
]
industry = industry[relevant_columns]

industry['DEP'] = industry['DEP'].str.lstrip('0')

# Remplacer les lettres A ou B par des zéros
industry['DEP'] = industry['DEP'].str.replace('A', '0').str.replace('B', '0')

industry["DEP"] = industry["DEP"].astype(int)

geographic2["CODGEO"] = geographic2["CODGEO"].astype(int) ## ATTENTION CHANGEMENT SUR LE VRAI DF PREPROCESSING

full_data = industry.merge(geographic2, how="left", left_on = "CODGEO", right_on="CODGEO")

top_industry = full_data.sort_values(by=["Sum"], ascending=False).head(10)

top_industry_names = top_industry["LIBGEO"].values.tolist()
top_industry_lons = top_industry["longitude"].values.tolist()
top_industry_lats = top_industry["latitude"].values.tolist()

top_industry = full_data.sort_values(by=["Sum"], ascending=False).head(10)

lons = full_data["longitude"].values.tolist()
lats = full_data["latitude"].values.tolist()
size = (full_data["Sum"]/5).values.tolist()

from mpl_toolkits.basemap import Basemap

!pip install matplotlib
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
import matplotlib.colors as colors # Import the colors module

# Creating new plot
plt.figure(figsize=(20,20))
# Load map of France
map = Basemap(projection='lcc',
            lat_0=46.2374,
            lon_0=2.375,
            resolution='h',
            llcrnrlon=-4.76, llcrnrlat=41.39,
            urcrnrlon=10.51, urcrnrlat=51.08)

# Draw parallels.
parallels = np.arange(40.,52,2.)
map.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)
# Draw meridians
meridians = np.arange(-6.,10.,2.)
map.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)

map.drawcoastlines()
map.drawcountries()
map.drawmapboundary()
map.drawrivers()

# Draw scatter plot with all cities
x,y = map(lons, lats)
map.scatter(x, y, s=size, alpha=0.6, c=size, norm=colors.LogNorm(vmin=1, vmax=max(size)), cmap='hsv') # Use the imported colors module
map.colorbar(location="bottom", pad="4%")

# Draw scatter plot of cities with highiest number of workplaces
x1, y1 = map(top_industry_lons, top_industry_lats)
map.scatter(x1, y1, c="black")

for i in range(len(top_industry_names)):
    plt.annotate(top_industry_names[i], xy=(map(top_industry_lons[i] + 0.25,  top_industry_lats[i])), fontsize=25)

plt.title("Les plus gros bassins d'entreprises en France métropolitaine", fontsize=40, fontweight='bold', y=1.05)

plt.show()

"""# Modèle - Forêt Aléatoire

## Simple

### Sans optimisation
"""

salaire_FA = salaire.drop(['CODGEO', 'LIBGEO'],axis=1)

# Séparation de la variable cible
feats=salaire_FA.drop(['salaire'],axis=1)
target=salaire_FA['salaire']

from sklearn.model_selection import train_test_split

# Création d'un jeu d'entrainement et de test
X_train, X_test, y_train, y_test = train_test_split(feats, target, test_size=0.25, random_state = 42)

# Vérification des dimensions
print('Dim train X : ', X_train.shape)
print ('Dim train y : ', y_train.shape)
print('Dim test X : ', X_test.shape)
print ('Dim test y : ', y_test.shape)

# Normalisation (entre 0 et 1)
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

X_train = scaler.fit_transform(X_train)

X_test = scaler.transform(X_test)

# Score R² du jeu d'entrainement et de test
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(X_train, y_train)

print('Score sur ensemble train', model.score(X_train, y_train))
print('Score sur ensemble test', model.score(X_test, y_test))

"""


*   **On constate  que le score R² est très élevé, il est supérieur à 0.90. Le R2 du jeu d'entrainement est supérieur à celui du jeu de test. On remarque aussi qu'ils sont presque équivalents.**


*   **On peut en conclure que le modèle fait de l'overfitting.**





"""

# Faire des prédictions sur les données de test
y_pred = model.predict(X_test)

# Détermination des métriques
from sklearn.metrics import mean_absolute_error, mean_squared_error

mse_forest = mean_squared_error(y_test, y_pred)
mae_forest = mean_absolute_error(y_test, y_pred)
rmse_forest = np.sqrt(mse_forest)

print('MSE_Forest:',mse_forest)
print('MAE_Forest:',mae_forest)
print('RMSE_Forest:',rmse_forest)

"""

*   **Les valeurs des métriques sont très faibles. Cela est du à l'important overfitting du modèle**



"""

# Calculer les résidus
residuals = y_test - y_pred

from scipy import stats

# Créer un subplot 2x2
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))

# Graphique de dispersion des résidus avec ligne horizontale à y=0
sns.scatterplot(x=y_pred, y=residuals, ax=axes[0, 0])
axes[0, 0].set_title("Graphique de Dispersion des Résidus")
axes[0, 0].set_xlabel("Prédictions")
axes[0, 0].set_ylabel("Résidus")
axes[0, 0].axhline(y=0, color='r', linestyle='--')

# Histogramme des résidus
sns.histplot(residuals, ax=axes[0, 1], kde=True)
axes[0, 1].set_title("Histogramme des Résidus")
axes[0, 1].set_xlabel("Résidus")

# Comparaison entre les Valeurs Réelles et Prédites avec ligne diagonale en rouge
sns.scatterplot(x=y_test, y=y_pred, ax=axes[1, 0])
axes[1, 0].set_title("Comparaison Valeurs Réelles vs. Prédites")
axes[1, 0].set_xlabel("Valeurs Réelles")
axes[1, 0].set_ylabel("Prédictions")
axes[1, 0].plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red')

# QQ plot des résidus
stats.probplot(residuals, plot=axes[1, 1])
axes[1, 1].set_title("QQ Plot des Résidus")

# Ajuster les espaces entre les sous-graphiques
plt.tight_layout()

# Afficher le subplot
plt.show()

"""*   **On constate qu'il y a une dispersion modérée des résidus entre les valeurs prédites et réelles. Plus la valeurs à prédire est élevée plus le résidu est important.**
*   **On voit aussi que répartition des erreurs suit une courbe gaussienne centrée sur 0.**
*   **Sur le QQplot des résidus, on remarque que la courbe ne suit pas totalement la droite, surtout aux extrèmités.**

### Avec optimisation
"""

# Matrice de corrélation
correlation_original = salaire_FA.corr()

# Afficher la heatmap avec seaborn pour la matrice de corrélation
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_original, annot=True, cmap='coolwarm', fmt=".1f")
plt.title('Matrice de corrélation du DataFrame original')
plt.show()

# Drop des variables trop corrélées avec la variable cible ( > 0.9)
salaire_FA_opt = salaire_FA.drop(['salaire_cadre','salaire_femme','salaire_homme','salaire_cadre_homme','salaire_26-50','salaire_+50','salaire_26-50_femme','salaire_+50_femme','salaire_26-50_homme','salaire_+50_homme'],axis=1)

# Séparation de la variable cible
feats=salaire_FA_opt.drop(['salaire'],axis=1)
target=salaire_FA_opt['salaire']

# Création d'un jeu d'entrainement et de test
X_train1, X_test1, y_train1, y_test1 = train_test_split(feats, target, test_size=0.25, random_state = 42)

# Normalisation (entre 0 et 1)
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

X_train = scaler.fit_transform(X_train1)

X_test = scaler.transform(X_test1)

# Score R² du jeu d'entrainement et de test
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(max_depth=7)
model.fit(X_train1, y_train1)

print('Score sur ensemble train', model.score(X_train1, y_train1))
print('Score sur ensemble test', model.score(X_test1, y_test1))

# Faire des prédictions sur les données de test
y_pred1 = model.predict(X_test1)

# Détermination des métriques
from sklearn.metrics import mean_absolute_error, mean_squared_error

mse_forest = mean_squared_error(y_test1, y_pred1)
mae_forest = mean_absolute_error(y_test1, y_pred1)
rmse_forest = np.sqrt(mse_forest)

print('MSE_Forest:',mse_forest)
print('MAE_Forest:',mae_forest)
print('RMSE_Forest:',rmse_forest)

"""

*   **Les valeurs des métriques sont plutot faibles.**
*   **En particulier le MSE, cela indique que les prédictions du modèle sont, en moyenne, très proches des valeurs réelles.**
*   **Le MAE et RMSE ont le même ordre de grandeur, leurs valeurs ne sont pas très importantes mais elles ne restent pas négligeables. Cela montre qu'il y'a un certain nombre d'erreur moyennement importante.**



"""

# Calculer les résidus
residuals1 = y_test1 - y_pred1

from scipy import stats

# Créer un subplot 2x2
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))

# Graphique de dispersion des résidus avec ligne horizontale à y=0
sns.scatterplot(x=y_pred1, y=residuals1, ax=axes[0, 0])
axes[0, 0].set_title("Graphique de Dispersion des Résidus")
axes[0, 0].set_xlabel("Prédictions")
axes[0, 0].set_ylabel("Résidus")
axes[0, 0].axhline(y=0, color='r', linestyle='--')

# Histogramme des résidus
sns.histplot(residuals1, ax=axes[0, 1], kde=True)
axes[0, 1].set_title("Histogramme des Résidus")
axes[0, 1].set_xlabel("Résidus")

# Comparaison entre les Valeurs Réelles et Prédites avec ligne diagonale en rouge
sns.scatterplot(x=y_test1, y=y_pred1, ax=axes[1, 0])
axes[1, 0].set_title("Comparaison Valeurs Réelles vs. Prédites")
axes[1, 0].set_xlabel("Valeurs Réelles")
axes[1, 0].set_ylabel("Prédictions")
axes[1, 0].plot([min(y_test1), max(y_test1)], [min(y_test1), max(y_test1)], linestyle='--', color='red')

# QQ plot des résidus
stats.probplot(residuals1, plot=axes[1, 1])
axes[1, 1].set_title("QQ Plot des Résidus")

# Ajuster les espaces entre les sous-graphiques
plt.tight_layout()

# Afficher le subplot
plt.show()

"""##  Avec discrétisation"""

salaire_box = salaire_FA
salaire_box['salaire'] = salaire_box['salaire'].astype(str)

from pandas.api.types import CategoricalDtype

# Sélectionner toutes les colonnes numériques sauf la variable cible
variables_numeriques = salaire_box.drop(['salaire'],axis=1)

# Boucle sur chaque colonne pour la discrétisation
for col in variables_numeriques:
    # Discrétisation en 5 intervalles
    salaire_box[f'{col}_discretise'] = pd.cut(salaire_box[col], bins=5)
    # Convertir la variable discrétisée en type catégoriel
    salaire_box[f'{col}_discretise'] = salaire_box[f'{col}_discretise'].astype(CategoricalDtype(ordered=True))


# Sélectionner les colonnes discrétisées
colonnes_discretisees = [col for col in salaire_box.columns if '_discretise' in col]

# Extraire uniquement les colonnes sélectionnées
salaire_disc = salaire_box[colonnes_discretisees]

salaire_disc.head()

label_pos = salaire_FA.columns[25:]
salaire0 = salaire_FA.drop(label_pos, axis=1)

cible = salaire_box['salaire']
table = pd.concat([salaire_disc,cible],axis=1)

# Encodage des variables catégorielles

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()

columns = table.columns

for column in columns:
    if column != "salaire":
        table[column] = encoder.fit_transform(table[column])

# Matrice de corrélation
correlation_original = table.corr()

# Afficher la heatmap avec seaborn pour la matrice de corrélation
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_original, annot=True, cmap='coolwarm', fmt=".1f")
plt.title('Matrice de corrélation du DataFrame original')
plt.show()

# Drop des variables trop corrélées avec la variable cible ( > 0.9)
table = table.drop(['salaire_femme_discretise','salaire_26-50_discretise','salaire_26-50_femme_discretise','salaire_26-50_homme_discretise'],axis=1)

# Matrice de corrélation
correlation_original = table.corr()

# Afficher la heatmap avec seaborn pour la matrice de corrélation
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_original, annot=True, cmap='coolwarm', fmt=".1f")
plt.title('Matrice de corrélation du DataFrame original')
plt.show()

# Séparation de la variable cible/features

feats=table.drop(['salaire'],axis=1)
target=table['salaire']

# Création d'un jeu d'entrainement et de test
X_train2, X_test2, y_train2, y_test2 = train_test_split(feats, target, random_state = 42)

# Vérification des dimensions
print('Dim train X : ', X_train2.shape)
print ('Dim train y : ', y_train2.shape)
print('Dim test X : ', X_test2.shape)
print ('Dim test y : ', y_test2.shape)

# Score R² du jeu d'entrainement et de test
from sklearn.ensemble import RandomForestRegressor


rf = RandomForestRegressor(verbose = 1, random_state=42,max_depth=9)
rf.fit(X_train2, y_train2)

print('Score sur ensemble train', rf.score(X_train2, y_train2))
print('Score sur ensemble test', rf.score(X_test2, y_test2))

# Faire des prédictions sur les données de test
y_pred2 = rf.predict(X_test2)

# Détermination des métriques
from sklearn.metrics import mean_absolute_error, mean_squared_error

mse_forest = mean_squared_error(y_test2, y_pred2)
mae_forest = mean_absolute_error(y_test2, y_pred2)
rmse_forest = np.sqrt(mse_forest)

print('MSE_Forest:',mse_forest)
print('MAE_Forest:',mae_forest)
print('RMSE_Forest:',rmse_forest)

y_test2 = pd.to_numeric(y_test2)
y_pred2 = pd.to_numeric(y_pred2)

residuals2 = y_test2 - y_pred2

# Créer un subplot 2x2
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))

# Graphique de dispersion des résidus avec ligne horizontale à y=0
sns.scatterplot(x=y_pred2, y=residuals2, ax=axes[0, 0])
axes[0, 0].set_title("Graphique de Dispersion des Résidus")
axes[0, 0].set_xlabel("Prédictions")
axes[0, 0].set_ylabel("Résidus")
axes[0, 0].axhline(y=0, color='r', linestyle='--')

# Histogramme des résidus
sns.histplot(residuals2, ax=axes[0, 1], kde=True)
axes[0, 1].set_title("Histogramme des Résidus")
axes[0, 1].set_xlabel("Résidus")

# Comparaison entre les Valeurs Réelles et Prédites avec ligne diagonale en rouge
sns.scatterplot(x=y_test2, y=y_pred2, ax=axes[1, 0])
axes[1, 0].set_title("Comparaison Valeurs Réelles vs. Prédites")
axes[1, 0].set_xlabel("Valeurs Réelles")
axes[1, 0].set_ylabel("Prédictions")
axes[1, 0].plot([min(y_test2), max(y_test2)], [min(y_test2), max(y_test2)], linestyle='--', color='red')

# QQ plot des résidus
stats.probplot(residuals2, plot=axes[1, 1])
axes[1, 1].set_title("QQ Plot des Résidus")

# Ajuster les espaces entre les sous-graphiques
plt.tight_layout()

# Afficher le subplot
plt.show()

# Visualisation des features importances
coefficients = rf.feature_importances_
feature_importance = pd.DataFrame(coefficients,index = X_train2.columns, columns=['Coefficient'])

plt.figure(figsize=(20, 20))
feature_importance = feature_importance.sort_values(by='Coefficient', ascending=False)
feature_importance.head(5).plot(kind='bar', legend=None)

plt.title('Feature Importance dans la prédiction de salaire')
plt.xlabel('Features')
plt.ylabel('Coefficient (valeur absolue)')
plt.xticks(rotation=45, ha='right')
plt.grid(True)
plt.show();

# Affichage de l'arbre de décision du modèle de foret aléatoire

from sklearn.datasets import make_regression
X, y = make_regression(n_features=4, n_informative=2,
                       random_state=42, shuffle=False)

rf1 = RandomForestRegressor(random_state=42,max_depth=3)
rf1.fit(X, y)

print(rf1.predict([[0, 0, 0, 0]]))
from sklearn import tree
plt.figure(figsize=(15,8))
tree.plot_tree(rf.estimators_[1],max_depth=2)

"""## Avec variable ratio H/F"""

salaire_min = salaire_FA

dfpopu_0 = population[['CODGEO','NB','SEXE']]

dfpopu = dfpopu_0.groupby(['CODGEO','SEXE']).sum().reset_index()

dfpopu_tot = dfpopu_0[['CODGEO','NB']].groupby('CODGEO').sum()
dfpopu_tot = dfpopu_tot.rename(columns={'NB':'NB_totale'}).reset_index()

dfpopu_2 = dfpopu.merge(right = dfpopu_tot,on =('CODGEO'),how = "left")
dfpopu_2['Ratio'] = (dfpopu_2['NB']/dfpopu_2['NB_totale'])*100
dfpopu_3 = dfpopu_2[['CODGEO','Ratio']]

dfpopu_3['Ratio_H'] = dfpopu_3['Ratio'][::2]
dfpopu_3['Ratio_F'] = dfpopu_3['Ratio'][1::2]
dfpopu_3=dfpopu_3.drop('Ratio',axis=1)

dfpopu_H = dfpopu_3[['CODGEO','Ratio_H']]
dfpopu_H = dfpopu_H.dropna(axis=0)
dfpopu_F = dfpopu_3[['CODGEO','Ratio_F']]
dfpopu_F = dfpopu_F.dropna(axis=0)
dfpopu_final = dfpopu_H.merge(right = dfpopu_F,on ='CODGEO',how = "inner")

salaire3=salaire.drop(['LIBGEO'],axis=1)

df_ML = salaire3.merge(right = dfpopu_final,on ='CODGEO',how = "left")
df_ML.head()

# Suppresion des NaNs
df_ML = df_ML.dropna(axis=0, how="any")

# Matrice de corrélation
correlation_original = df_ML.corr()

# Afficher la heatmap avec seaborn pour la matrice de corrélation
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_original, annot=True, cmap='coolwarm', fmt=".1f")
plt.title('Matrice de corrélation du DataFrame original')
plt.show()

# Drop des variables trop corrélées avec la variable cible ( > 0.9)
df_ML = df_ML.drop(['CODGEO','salaire_employe','salaire_femme','salaire_homme','salaire_26-50','salaire_+50','salaire_26-50_femme','salaire_+50_femme','salaire_26-50_homme','salaire_+50_homme'],axis=1)

# Matrice de corrélation
correlation_original = df_ML.corr()

# Afficher la heatmap avec seaborn pour la matrice de corrélation
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_original, annot=True, cmap='coolwarm', fmt=".1f")
plt.title('Matrice de corrélation du DataFrame original')
plt.show()

# Séparation de la variable cible
feats=df_ML.drop(['salaire'],axis=1)
target=df_ML['salaire']

from sklearn.model_selection import train_test_split

# Création d'un jeu d'entrainement et de test
X_train3, X_test3, y_train3, y_test3 = train_test_split(feats, target, test_size=0.25, random_state = 42)

# Vérification des dimensions
print('Dim train X : ', X_train3.shape)
print ('Dim train y : ', y_train3.shape)
print('Dim test X : ', X_test3.shape)
print ('Dim test y : ', y_test3.shape)

# Normalisation (entre 0 et 1)
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

X_train = scaler.fit_transform(X_train3)

X_test = scaler.transform(X_test3)

# Score R² du jeu d'entrainement et de test
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(max_depth=6)
model.fit(X_train3, y_train3)

print('Score sur ensemble train', model.score(X_train3, y_train3))
print('Score sur ensemble test', model.score(X_test3, y_test3))

# Faire des prédictions sur les données de test
y_pred3 = model.predict(X_test3)

# Détermination des métriques
from sklearn.metrics import mean_absolute_error, mean_squared_error

mse_forest = mean_squared_error(y_test3, y_pred3)
mae_forest = mean_absolute_error(y_test3, y_pred3)
rmse_forest = np.sqrt(mse_forest)

print('MSE_Forest:',mse_forest)
print('MAE_Forest:',mae_forest)
print('RMSE_Forest:',rmse_forest)

"""**Les métriques sont assez faible, les erreurs seront donc limitées**"""

# Calculer les résidus
y_test3 = pd.to_numeric(y_test3)
y_pred3 = pd.to_numeric(y_pred3)

residuals3 = y_test3 - y_pred3

from scipy import stats

# Créer un subplot 2x2
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))

# Graphique de dispersion des résidus avec ligne horizontale à y=0
sns.scatterplot(x=y_pred3, y=residuals3, ax=axes[0, 0])
axes[0, 0].set_title("Graphique de Dispersion des Résidus")
axes[0, 0].set_xlabel("Prédictions")
axes[0, 0].set_ylabel("Résidus")
axes[0, 0].axhline(y=0, color='r', linestyle='--')

# Histogramme des résidus
sns.histplot(residuals3, ax=axes[0, 1], kde=True)
axes[0, 1].set_title("Histogramme des Résidus")
axes[0, 1].set_xlabel("Résidus")

# Comparaison entre les Valeurs Réelles et Prédites avec ligne diagonale en rouge
sns.scatterplot(x=y_test3, y=y_pred3, ax=axes[1, 0])
axes[1, 0].set_title("Comparaison Valeurs Réelles vs. Prédites")
axes[1, 0].set_xlabel("Valeurs Réelles")
axes[1, 0].set_ylabel("Prédictions")
axes[1, 0].plot([min(y_test3), max(y_test3)], [min(y_test3), max(y_test3)], linestyle='--', color='red')

# QQ plot des résidus
stats.probplot(residuals3, plot=axes[1, 1])
axes[1, 1].set_title("QQ Plot des Résidus")

# Ajuster les espaces entre les sous-graphiques
plt.tight_layout()

# Afficher le subplot
plt.show()

"""*   **On remarque que les résidus sont plutot compactes sur le graphique. Il y a par contre des outliers**
*   **L'histogramme des résidus montre une courbe gaussienne**

# Modèle - Régression linéaire 2

## Préparation du jeu de données et création d'une nouvelle variable "Region_code"
"""

#Création d'un DataFrame salaire2 identique au DF Salaire
salaire2 = salaire

#Observation de ce DF
salaire2.head()

#Observation approfondie de ce nouveau DataFrame
salaire2.info()

print(salaire['CODGEO'].tolist())

#Création d'une fonction qui permet d'attribuer 1 ou 0 en fonction des département Parisien
def check_codgeo(codgeo):
    return 1 if codgeo.startswith(('91', '92', '93', '94', '95', '75')) else 0

# Appliquer la fonction à la colonne CODGEO et créer une nouvelle colonne 'Region_Code'
salaire2['Region_Code'] = salaire2['CODGEO'].apply(check_codgeo)

#La nouvelle colonne a bien été créee
pd.DataFrame(salaire2['Region_Code'].head(10))

#Création des variables feature et d'une variable target
X=salaire2.drop(['salaire', 'CODGEO','LIBGEO'],axis=1)
y=salaire2['salaire']

#Séparation du jeu de données en Test et Train
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=123)

#Observation des dimenssions test et train
print('Dim X_train : ', X_train.shape)
print ('Dim y_train : ', y_train.shape)
print('Dim X_test : ', X_test.shape)
print ('Dim y_test : ', y_test.shape)

#Appel du modèle de regréssion linéaire et travail du modèle sur les dimenssions train
from sklearn.linear_model import LinearRegression

# Création et entraînement du modèle
reg = LinearRegression()
reg.fit(X_train, y_train)  # X_train sont vos variables indépendantes, y_train est la variable dépendante

# Évaluation du modèle
print('Score sur ensemble train logistic', reg.score(X_train, y_train))
print('Score sur ensemble test logistic', reg.score(X_test, y_test))

# Affiche l'ordonnée à l'origine
print(reg.intercept_)
# Affiche le coéfficient directeur
print(reg.coef_[0])

"""On peut interpréter que quand X augmente d'une unité, en moyenne y augmentera de 0.0197
Le coefficient direcreur est quant à lui négatif est relativement proche de 0. Donc à mesure que X gagne en unité, la pente de la droite devrait descendre mais assez faiblement.
"""

#Prédictions

predictions = reg.predict(X_test)
erreurs = predictions - y_test

erreurs

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Prédictions sur les ensembles d'entraînement et de test
y_train_pred = reg.predict(X_train)
y_test_pred = reg.predict(X_test)

# Calcul des métriques
mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
rmse_train = np.sqrt(mse_train)
rmse_test = np.sqrt(mse_test)
mae_train = mean_absolute_error(y_train, y_train_pred)
mae_test = mean_absolute_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

# Affichage des métriques
print("Train Set Metrics:")
print("MAE:", mae_train)
print("MSE:", mse_train)
print("RMSE:", rmse_train)
print("R²:", r2_train)

print("\nTest Set Metrics:")
print("MAE:", mae_test)
print("MSE:", mse_test)
print("RMSE:", rmse_test)
print("R²:", r2_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error

# Calcul des métriques de performance supplémentaires
mae = mean_absolute_error(y_test, predictions)
mse = mean_squared_error(y_test, predictions)
rmse = np.sqrt(mse)
r2_train = r2_score(y_test, predictions)

print('Mean Absolute Error (MAE):', mae)
print('Mean Squared Error (MSE):', mse)
print('Root Mean Squared Error (RMSE):', rmse)
print('R² :', r2_train)

plt.figure(figsize=(10, 6))
plt.scatter(predictions, erreurs, edgecolors=(0, 0, 0))
plt.axhline(y=0, color='r', linestyle='--', linewidth=2)
plt.xlabel('Valeurs prédites')
plt.ylabel('Résidus')
plt.title('Résidus vs Valeurs prédites')
plt.tight_layout()
plt.show()

plt.hist(erreurs,bins=50)
plt.grid(True)
plt.show()

plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_test, y=predictions, edgecolor=(0, 0, 0))
sns.regplot(x=y_test, y=predictions, scatter=False, color='red', line_kws={'linewidth': 2})
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)
plt.xlabel('Valeurs réelles')
plt.ylabel('Valeurs prédites')
plt.title('Valeurs prédites vs Valeurs réelles');

# Récupérer les coefficients de régression du modèle
coefficients = reg.coef_

# Associer chaque coefficient à son nom de variable correspondant
coefficients_df = pd.DataFrame({'Variable': X.columns, 'Coefficient': coefficients})

# Trier les coefficients par valeur absolue pour mettre en évidence les variables ayant le plus d'impact
coefficients_df['Absolute_Coefficient'] = coefficients_df['Coefficient'].abs()
coefficients_df = coefficients_df.sort_values(by='Absolute_Coefficient', ascending=False)

# Tracer le barplot des coefficients
plt.figure(figsize=(10, 6))
plt.barh(coefficients_df['Variable'], coefficients_df['Coefficient'], color='skyblue')
plt.xlabel('Coefficient de régression')
plt.ylabel('Variable')
plt.title('Impact des variables sur le modèle de régression linéaire')
plt.show()

#Création des variables feature et d'une variable target
X=salaire2.drop(['salaire', 'CODGEO', 'LIBGEO','salaire_26-50','salaire_homme', 'salaire_femme','salaire_26-50_femme','salaire_26-50_homme','salaire_+50'],axis=1)
y=salaire2['salaire']

#Séparation du jeu de données en Test et Train
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=123)

#Appel du modèle de regréssion linéaire et travail du modèle sur les dimenssions train
from sklearn.linear_model import LinearRegression

# Création et entraînement du modèle
reg = LinearRegression()
reg.fit(X_train, y_train)  # X_train sont vos variables indépendantes, y_train est la variable dépendante

# Évaluation du modèle
print('Score sur ensemble train logistic', reg.score(X_train, y_train))
print('Score sur ensemble test logistic', reg.score(X_test, y_test))

#Observation des dimenssions test et train
print('Dim X_train : ', X_train.shape)
print ('Dim y_train : ', y_train.shape)
print('Dim X_test : ', X_test.shape)
print ('Dim y_test : ', y_test.shape)

#Prédictions

predictions = reg.predict(X_test)
erreurs = predictions - y_test

erreurs

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Prédictions sur les ensembles d'entraînement et de test
y_train_pred = reg.predict(X_train)
y_test_pred = reg.predict(X_test)

# Calcul des métriques
mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
rmse_train = np.sqrt(mse_train)
rmse_test = np.sqrt(mse_test)
mae_train = mean_absolute_error(y_train, y_train_pred)
mae_test = mean_absolute_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

# Affichage des métriques
print("Train Set Metrics:")
print("MAE:", mae_train)
print("MSE:", mse_train)
print("RMSE:", rmse_train)
print("R²:", r2_train)

print("\nTest Set Metrics:")
print("MAE:", mae_test)
print("MSE:", mse_test)
print("RMSE:", rmse_test)
print("R²:", r2_test)

plt.figure(figsize=(10, 6))
plt.scatter(predictions, erreurs, edgecolors=(0, 0, 0))
plt.axhline(y=0, color='r', linestyle='--', linewidth=2)
plt.xlabel('Valeurs prédites')
plt.ylabel('Résidus')
plt.title('Résidus vs Valeurs prédites')
plt.tight_layout()
plt.show()

plt.hist(erreurs,bins=50)
plt.grid(True)
plt.show()

plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_test, y=predictions, edgecolor=(0, 0, 0))
sns.regplot(x=y_test, y=predictions, scatter=False, color='red', line_kws={'linewidth': 2})
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)
plt.xlabel('Valeurs réelles')
plt.ylabel('Valeurs prédites')
plt.title('Valeurs prédites vs Valeurs réelles');

# Récupérer les coefficients de régression du modèle
coefficients = reg.coef_

# Associer chaque coefficient à son nom de variable correspondant
coefficients_df = pd.DataFrame({'Variable': X.columns, 'Coefficient': coefficients})

# Trier les coefficients par valeur absolue pour mettre en évidence les variables ayant le plus d'impact
coefficients_df['Absolute_Coefficient'] = coefficients_df['Coefficient'].abs()
coefficients_df = coefficients_df.sort_values(by='Absolute_Coefficient', ascending=False)

# Tracer le barplot des coefficients
plt.figure(figsize=(10, 6))
plt.barh(coefficients_df['Variable'], coefficients_df['Coefficient'], color='skyblue')
plt.xlabel('Coefficient de régression')
plt.ylabel('Variable')
plt.title('Impact des variables sur le modèle de régression linéaire')
plt.show()

"""#

# Modèle - Régression linaire 1

## Modèle de regression linéaire : Catégories socio-professionnelles

###Étape 1 : Sélection des caractéristiques (features) et division des données
"""

#Pour simplifier, nous allons commencer avec une sélection plus restreinte pour voir l'influence de quelques variables distinctes sur le salaire moyen.

features = [
    'salaire_cadre_femme', 'salaire_cadre_moyen_femme',
    'salaire_employe_femme', 'salaire_travailleur_femme',
    'salaire_cadre_homme', 'salaire_cadre_moyen_homme',
    'salaire_employe_homme', 'salaire_travailleur_homme'
]
X = salaire[features]
y = salaire['salaire']

# Diviser les données en ensemble d'entraînement et de test

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""###Étape 2 : Entraînement du modèle"""

# Création du modèle linéaire
model = LinearRegression()

# Entraînement du modèle
model.fit(X_train, y_train)

"""###Étape 3 : Évaluation du modèle"""

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Prédictions sur les ensembles d'entraînement et de test
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# Calcul des métriques
mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
rmse_train = np.sqrt(mse_train)
rmse_test = np.sqrt(mse_test)
mae_train = mean_absolute_error(y_train, y_train_pred)
mae_test = mean_absolute_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

# Affichage des métriques
print("Train Set Metrics:")
print("MAE:", mae_train)
print("MSE:", mse_train)
print("RMSE:", rmse_train)
print("R²:", r2_train)

print("\nTest Set Metrics:")
print("MAE:", mae_test)
print("MSE:", mse_test)
print("RMSE:", rmse_test)
print("R²:", r2_test)

"""### Étape 4 : Interprétation des Métriques de performance :

L'interprétation des métriques de performance d'un modèle de régression linéaire, telles que l'Erreur Quadratique Moyenne (MSE) et le Coefficient de Détermination (R²), permet de comprendre à quel point le modèle est efficace pour prédire la variable cible. Voici comment nous pouvons interpréter les métriques obtenues pour notre modèle :

#### R² (Coefficient de Détermination)
**- R² du Train Set: 0.922** - Ce qui signifie que le modèle explique environ 92.2% de la variabilité des salaires dans les données d'entraînement. C'est à dire le modèle est capable de comprendre et de suivre très bien la manière dont les salaires varient selon les facteurs étudiés dans cet ensemble de données.

**- R² du Test Set: 0.906 -** Ce qui indique que le modèle explique environ 90.6% de la variabilité des salaires dans les données de test. C'est un peu moins que sur l'ensemble d'entraînement, mais c'est normal et montre que le modèle est capable de bien généraliser ce qu'il a appris à de nouvelles données.


### MSE et RMSE (Erreur Quadratique Moyenne et Racine de l'Erreur Quadratique Moyenne)

**Train MSE: 0.538**

**Test MSE: 0.527**

**Train RMSE: 0.726**

**Test RMSE: 0.727**

Ces valeurs montrent que les erreurs de prédiction du modèle, tant sur l'ensemble d'entraînement que de test, sont assez proches, indiquant une bonne consistance du modèle.



### MAE (Erreur Absolue Moyenne)

**Train MAE: 0.525**

**Test MAE: 0.526**

Le MAE proche entre l'ensemble d'entraînement et de test suggère également que le modèle a une bonne stabilité dans ses prédictions.


--------------------------------------


En résumé, ce modèle de prédiction des salaires est performant, précis, et fiable, avec une bonne capacité à généraliser ses prédictions à de nouvelles données. Les erreurs sont relativement petites et similaires entre les ensembles d'entraînement et de test, ce qui est un bon signe de sa stabilité et de sa fiabilité.

### Étape 5 : Visualisation des features importances

Pour visualiser les featues importances dans un modèle de régression linéaire, nous utilisons généralement les coefficients du modèle pour évaluer l'impact relatif de chaque caractéristique sur la prédiction de la variable cible.

**Plus le coefficient est grand en valeur absolue, plus l'influence de la caractéristique correspondante sur la prédiction est importante.**
"""

# Obtention des coefficients et visualisation des features importances
coefficients = model.coef_
feature_importance = pd.DataFrame(coefficients, index=features, columns=['Coefficient'])

plt.figure(figsize=(10, 10))
feature_importance.sort_values(by='Coefficient', ascending=False).plot(kind='bar', legend=None)
plt.title('Feature Importance dans la prédiction de salaire')
plt.xlabel('Features')
plt.ylabel('Coefficient (valeur absolue')
plt.xticks(rotation=45, ha='right')
plt.grid(True)
plt.show();

"""La visualisation précédente des coefficients de régression montre quelles caractéristiques sont les plus influentes dans le modèle. Les caractéristiques avec des coefficients plus élevés ont une influence plus grande sur la prédiction du salaire.

**salaire_employe_femme**  montre l'influence la plus importante sur la prédiction du salaire.
**salaire_travailleur_homme** et **salaire_employe_homme** ont une influence modérée.
Les restes ont des influences relativement plus faibles par rapport aux autres caractéristiques.

En conclusion, le modèle de régression linéaire montre d'excellentes performances avec de bonnes métriques sur les ensembles de données d'entraînement et de test. Ce modèle peut être utilisé pour évaluer l'impact de différents facteurs sur les salaires à travers la France, ce qui peut aider à comprendre les distributions salariales.

### Étape 6 : Analyse du Scatterplot des Valeurs Réelles vs Prédites
"""

import numpy as np

# Scatterplot valeurs réelles vs prédictions
n_points = 100
target = np.random.rand(n_points) * 100  # Valeurs réelles
predictions = target + np.random.normal(0, 10, n_points)  # Prédictions avec un bruit gaussien

# Créer un DataFrame
data_example = pd.DataFrame({
    'target': target,
    'predictions': predictions
})

# Scatterplot valeurs réelles vs prédictions
plt.figure(figsize=(10, 6))
plt.scatter(data_example['target'], data_example['predictions'], alpha=0.5)
plt.title('Comparaison des valeurs réelles vs prédites')
plt.xlabel('Valeurs réelles')
plt.ylabel('Valeurs prédites')
plt.plot([min(data_example['target']), max(data_example['target'])],
         [min(data_example['target']), max(data_example['target'])],
         color='red', linestyle='--')
plt.show()

"""***Analyse du Scatterplot des Valeurs Réelles vs Prédites***

Le scatterplot ci-dessus compare les valeurs réelles aux valeurs prédites par notre modèle de régression.

 ***Interprétation :***

- **Bonne Performance Générale** :
  - Le modèle montre une bonne performance générale, capturant bien la tendance des données réelles. La proximité des points à la ligne rouge est un signe positif.

- **Erreurs de Prédiction** :
  - La dispersion autour de la ligne rouge révèle qu'il y a des erreurs de prédiction, ce qui est attendu dans la plupart des modèles de régression. Toutefois, ces erreurs ne semblent pas systématiquement biaisées dans une direction, ce qui est encourageant.

- **Prédictions aux Extrêmes** :
  - L'augmentation de la dispersion aux extrêmes suggère que le modèle pourrait bénéficier d'un ajustement supplémentaire pour améliorer les prédictions dans ces plages de valeurs.

En conclusion, notre modèle montre des résultats prometteurs avec une bonne correspondance générale entre les valeurs réelles et prédites, malgré quelques erreurs à corriger.

###Étape 7 : Analyse de l'Histogramme des Résidus
"""

# Calcul des résidus
residuals = data_example['target'] - data_example['predictions']

# Histogramme des résidus
plt.figure(figsize=(10, 6))
plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)
plt.title('Histogramme des résidus')
plt.xlabel('Résidus')
plt.ylabel('Fréquence')
plt.show()

"""L'histogramme des résidus ci-dessus montre la distribution des erreurs de prédiction.

***Interprétation :***

Le modèle montre une bonne performance générale avec une concentration des résidus autour de zéro, indiquant des prédictions précises dans la majorité des cas.
Cependant, l'asymétrie et la dispersion relativement large des résidus suggèrent qu'il y a des erreurs à corriger pour certaines observations.

##Modèle de regression linéaire : Tranches d'âge

Pour appliquer le même processus d'analyse et de modélisation avec les nouvelles features salaire_18-25_femme, salaire_26-50_femme, salaire_+50_femme, salaire_18-25_homme, salaire_26-50_homme, salaire_+50_homme, nous allons suivre les étapes suivantes :

Sélectionner les nouvelles features.
Diviser les données en ensembles d'entraînement et de test.
Entraîner un modèle de régression linéaire avec ces nouvelles features.
Calculer et afficher les métriques de performance.
Visualiser les coefficients des features pour interpréter leur importance.
Créer des visualisations des valeurs réelles vs prédites et des résidus.
"""

# Sélection des nouvelles features
new_features = [
    'salaire_18-25_femme', 'salaire_26-50_femme', 'salaire_+50_femme',
    'salaire_18-25_homme', 'salaire_26-50_homme', 'salaire_+50_homme'
]
X_new = salaire[new_features]
y = salaire['salaire']

# Diviser les données en ensemble d'entraînement et de test
X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y, test_size=0.2, random_state=42)

# Création du modèle linéaire
model_new = LinearRegression()

# Entraînement du modèle
model_new.fit(X_train_new, y_train_new)

# Prédictions sur les ensembles d'entraînement et de test
y_train_pred_new = model_new.predict(X_train_new)
y_test_pred_new = model_new.predict(X_test_new)

# Calcul des métriques
mse_train_new = mean_squared_error(y_train_new, y_train_pred_new)
mse_test_new = mean_squared_error(y_test_new, y_test_pred_new)
rmse_train_new = np.sqrt(mse_train_new)
rmse_test_new = np.sqrt(mse_test_new)
mae_train_new = mean_absolute_error(y_train_new, y_train_pred_new)
mae_test_new = mean_absolute_error(y_test_new, y_test_pred_new)
r2_train_new = r2_score(y_train_new, y_train_pred_new)
r2_test_new = r2_score(y_test_new, y_test_pred_new)

# Affichage des métriques
print("Train Set Metrics:")
print("MAE:", mae_train_new)
print("MSE:", mse_train_new)
print("RMSE:", rmse_train_new)
print("R²:", r2_train_new)

print("\nTest Set Metrics:")
print("MAE:", mae_test_new)
print("MSE:", mse_test_new)
print("RMSE:", rmse_test_new)
print("R²:", r2_test_new)

# Obtention des coefficients et visualisation des features importances
coefficients_new = model_new.coef_
feature_importance_new = pd.DataFrame(coefficients_new, index=new_features, columns=['Coefficient'])

plt.figure(figsize=(10, 6))
feature_importance_new.sort_values(by='Coefficient', ascending=False).plot(kind='bar', legend=None)
plt.title('Feature Importance dans la prédiction de salaire')
plt.xlabel('Features')
plt.ylabel('Coefficient (valeur absolue)')
plt.xticks(rotation=45, ha='right')
plt.grid(True)
plt.show()

# Scatterplot valeurs réelles vs prédictions
plt.figure(figsize=(10, 6))
plt.scatter(y_test_new, y_test_pred_new, alpha=0.5)
plt.title('Comparaison des valeurs réelles vs prédites')
plt.xlabel('Valeurs réelles')
plt.ylabel('Valeurs prédites')
plt.plot([min(y_test_new), max(y_test_new)],
         [min(y_test_new), max(y_test_new)],
         color='red', linestyle='--')
plt.show()

# Calcul des résidus
residuals_new = y_test_new - y_test_pred_new

# Histogramme des résidus
plt.figure(figsize=(10, 6))
plt.hist(residuals_new, bins=30, edgecolor='black', alpha=0.7)
plt.title('Histogramme des résidus')
plt.xlabel('Résidus')
plt.ylabel('Fréquence')
plt.show()

"""### Analyse des Résultats Obtenus

### Métriques de Performance

#### Train Set Metrics
- **MAE (Mean Absolute Error)**: 0.1378
- **MSE (Mean Squared Error)**: 0.0433
- **RMSE (Root Mean Squared Error)**: 0.2081
- **R² (Coefficient de Détermination)**: 0.9936

#### Test Set Metrics
- **MAE (Mean Absolute Error)**: 0.1393
- **MSE (Mean Squared Error)**: 0.0363
- **RMSE (Root Mean Squared Error)**: 0.1906
- **R² (Coefficient de Détermination)**: 0.9935

### Interprétation des Métriques

- **MAE** : La valeur de l'erreur absolue moyenne (MAE) est très faible pour les ensembles d'entraînement et de test, ce qui indique que les prédictions du modèle sont, en moyenne, très proches des valeurs réelles.
- **MSE et RMSE** : Ces valeurs sont également très faibles, ce qui confirme la précision du modèle. Le RMSE étant proche de zéro montre que les grandes erreurs sont rares.
- **R²** : Un R² de 0.9936 pour l'ensemble d'entraînement et de 0.9935 pour l'ensemble de test indique que le modèle explique environ 99.36% et 99.35% de la variance dans les données, respectivement. C'est un très bon indicateur de la performance du modèle.

#### 1. Feature Importance

Le graphique des features montre les coefficients des différentes catégories de salaires dans la prédiction du salaire moyen :

- **salaire_26-50_homme** et **salaire_26-50_femme** ont les coefficients les plus élevés, indiquant qu'ils sont les plus influents dans la prédiction du salaire moyen.
- **salaire_+50_homme** et **salaire_+50_femme** ont une importance modérée.
- **salaire_18-25_homme** et **salaire_18-25_femme** ont les coefficients les plus bas, suggérant qu'ils ont une influence moindre sur la prédiction.

#### 2. Scatterplot des Valeurs Réelles vs Prédictions

- La majorité des points se situent très près de la ligne rouge pointillée, indiquant que les prédictions du modèle sont très proches des valeurs réelles.
- Cela suggère que le modèle a une très bonne performance en termes de précision.

#### 3. Histogramme des Résidus

- La distribution des résidus est centrée autour de zéro, ce qui est idéal.
- La plupart des résidus sont très proches de zéro, indiquant que les erreurs de prédiction sont généralement faibles.
- La forme en cloche symétrique de l'histogramme suggère que les erreurs sont réparties de manière normale, sans biais systématique.

### Conclusion

- Le modèle de régression linéaire utilisant les features sélectionnées performe exceptionnellement bien pour prédire le salaire moyen, avec des métriques de performance très élevées et des erreurs de prédiction faibles.
- Les features les plus influentes sont les salaires des hommes et des femmes âgés de 26 à 50 ans.
- La distribution des résidus et les scatterplots montrent que les prédictions du modèle sont robustes et précises.

Ces résultats indiquent que le modèle actuel est très efficace pour prédire les salaires moyens basés sur les catégories de salaires par âge et sexe.

##Modèle de regression linéaire : Catégories socio-professionnelles et tranches d'âge
"""

# Liste complète des nouvelles features
all_features = [
    'salaire_18-25_femme', 'salaire_26-50_femme', 'salaire_+50_femme',
    'salaire_18-25_homme', 'salaire_26-50_homme', 'salaire_+50_homme',
    'salaire_cadre_femme', 'salaire_cadre_moyen_femme',
    'salaire_employe_femme', 'salaire_travailleur_femme',
    'salaire_cadre_homme', 'salaire_cadre_moyen_homme',
    'salaire_employe_homme', 'salaire_travailleur_homme'
]

# Sélection des nouvelles features
X_all = salaire[all_features]
y = salaire['salaire']

# Diviser les données en ensemble d'entraînement et de test
X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X_all, y, test_size=0.2, random_state=42)

# Création du modèle linéaire
model_all = LinearRegression()

# Entraînement du modèle
model_all.fit(X_train_all, y_train_all)

# Prédictions sur les ensembles d'entraînement et de test
y_train_pred_all = model_all.predict(X_train_all)
y_test_pred_all = model_all.predict(X_test_all)

# Calcul des métriques
mse_train_all = mean_squared_error(y_train_all, y_train_pred_all)
mse_test_all = mean_squared_error(y_test_all, y_test_pred_all)
rmse_train_all = np.sqrt(mse_train_all)
rmse_test_all = np.sqrt(mse_test_all)
mae_train_all = mean_absolute_error(y_train_all, y_train_pred_all)
mae_test_all = mean_absolute_error(y_test_all, y_test_pred_all)
r2_train_all = r2_score(y_train_all, y_train_pred_all)
r2_test_all = r2_score(y_test_all, y_test_pred_all)

# Affichage des métriques
print("Train Set Metrics:")
print("MAE:", mae_train_all)
print("MSE:", mse_train_all)
print("RMSE:", rmse_train_all)
print("R²:", r2_train_all)

print("\nTest Set Metrics:")
print("MAE:", mae_test_all)
print("MSE:", mse_test_all)
print("RMSE:", rmse_test_all)
print("R²:", r2_test_all)

# Obtention des coefficients et visualisation des features importances
coefficients_all = model_all.coef_
feature_importance_all = pd.DataFrame(coefficients_all, index=all_features, columns=['Coefficient'])

plt.figure(figsize=(12, 8))
feature_importance_all.sort_values(by='Coefficient', ascending=False).plot(kind='bar', legend=None)
plt.title('Feature Importance dans la prédiction de salaire')
plt.xlabel('Features')
plt.ylabel('Coefficient (valeur absolue)')
plt.xticks(rotation=45, ha='right')
plt.grid(True)
plt.show()

# Scatterplot valeurs réelles vs prédictions
plt.figure(figsize=(10, 6))
plt.scatter(y_test_all, y_test_pred_all, alpha=0.5)
plt.title('Comparaison des valeurs réelles vs prédites')
plt.xlabel('Valeurs réelles')
plt.ylabel('Valeurs prédites')
plt.plot([min(y_test_all), max(y_test_all)],
         [min(y_test_all), max(y_test_all)],
         color='red', linestyle='--')
plt.show()

# Calcul des résidus
residuals_all = y_test_all - y_test_pred_all

# Histogramme des résidus
plt.figure(figsize=(10, 6))
plt.hist(residuals_all, bins=30, edgecolor='black', alpha=0.7)
plt.title('Histogramme des résidus')
plt.xlabel('Résidus')
plt.ylabel('Fréquence')
plt.show()

"""### Analyse des Résultats Obtenus

### Métriques de Performance

#### Train Set Metrics
- **MAE (Mean Absolute Error)**: 0.1298
- **MSE (Mean Squared Error)**: 0.0364
- **RMSE (Root Mean Squared Error)**: 0.1907
- **R² (Coefficient de Détermination)**: 0.9946

#### Test Set Metrics
- **MAE (Mean Absolute Error)**: 0.1319
- **MSE (Mean Squared Error)**: 0.0344
- **RMSE (Root Mean Squared Error)**: 0.1855
- **R² (Coefficient de Détermination)**: 0.9938

### Interprétation des Métriques

- **MAE** : La valeur de l'erreur absolue moyenne (MAE) est très faible pour les ensembles d'entraînement et de test, ce qui indique que les prédictions du modèle sont, en moyenne, très proches des valeurs réelles.
- **MSE et RMSE** : Ces valeurs sont également très faibles, ce qui confirme la précision du modèle. Le RMSE étant proche de zéro montre que les grandes erreurs sont rares.
- **R²** : Un R² de 0.9946 pour l'ensemble d'entraînement et de 0.9938 pour l'ensemble de test indique que le modèle explique environ 99.46% et 99.38% de la variance dans les données, respectivement. C'est un très bon indicateur de la performance du modèle.

#### 1. Feature Importance

Le graphique des features montre les coefficients des différentes catégories de salaires dans la prédiction du salaire moyen :

- **salaire_26-50_homme** et **salaire_26-50_femme** ont les coefficients les plus élevés, indiquant qu'ils sont les plus influents dans la prédiction du salaire moyen.
- **salaire_+50_homme** et **salaire_+50_femme** ont une importance modérée.


#### 2. Scatterplot des Valeurs Réelles vs Prédictions

- La majorité des points se situent très près de la ligne rouge pointillée, indiquant que les prédictions du modèle sont très proches des valeurs réelles.
- Cela suggère que le modèle a une très bonne performance en termes de précision.

#### 3. Histogramme des Résidus

- La distribution des résidus est centrée autour de zéro, ce qui est idéal.
- La plupart des résidus sont très proches de zéro, indiquant que les erreurs de prédiction sont généralement faibles.
- La forme en cloche symétrique de l'histogramme suggère que les erreurs sont réparties de manière normale, sans biais systématique.

### Conclusion

- Le modèle de régression linéaire utilisant les features sélectionnées performe exceptionnellement bien pour prédire le salaire moyen, avec des métriques de performance très élevées et des erreurs de prédiction faibles.
- Les features les plus influentes sont les salaires des hommes et des femmes âgés de 26 à 50 ans.
- La distribution des résidus et les scatterplots montrent que les prédictions du modèle sont robustes et précises.

Ces résultats indiquent que le modèle actuel est très efficace pour prédire les salaires moyens basés sur les catégories de salaires et tranche âge pour les deux sexs.

"""



"""# Modèle - Clustering (Salaires Catégories professionnelles)"""

# Merge entre salaire et geographic pour récupérer les départements
salaire_geo = pd.merge(salaire, geographic, on='CODGEO', how='inner')

print(salaire.shape)
print(salaire_geo.shape)
# print(salaire_geo.head())

from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

# Étape 1 : Calcule les salaires moyens par catégories professionnelles par nom de département
departements = salaire_geo.groupby('nom_département')[['salaire', 'salaire_cadre', 'salaire_cadre_moyen', 'salaire_employe', 'salaire_travailleur']].mean().reset_index()
print('Taille du Dataframe departements : ',departements.shape)

# -----------------------------------------------------------------------------------------
# On passe de 5 136 lignes du dataframe Saliare à 100 lignes agrégées par nom de département
# -------------------------------------------------------------------------------------------


# Étape 2 : Extraire les labels des départements pour les utiliser comme étiquettes dans le dendrogramme.
departements_labels = departements['nom_département'].tolist()
# Étape 3 : Créer la figure pour la visualisation
fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,18))
# Étape 4 : Extraire les valeurs des salaires et on convertit le DataFrame en un tableau NumPy de valeurs.
departements_salaire = departements.loc[:,['salaire','salaire_cadre','salaire_cadre_moyen','salaire_employe','salaire_travailleur']].values

# Étape 5 : Calculer la liaison hiérarchique
# Nous utilisons la méthode de Ward pour calculer les liaisons hiérarchiques
# entre les observations dans departements_salaire.
# La méthode de Ward minimise la variance totale au sein de chaque cluster.
mergings = linkage(departements_salaire, method='ward')

# # Étape 6 : Tracer le dendrogramme
# La fonction dendrogram trace un dendrogramme pour visualiser les résultats du clustering hiérarchique.
# labels=departements_labels associe les noms des départements aux feuilles du dendrogramme.
# orientation='right' oriente le dendrogramme horizontalement.
# color_threshold=4 colore les clusters en fonction d'un seuil de distance.

dend = dendrogram(mergings,
        labels=departements_labels,
        leaf_rotation=0,
        leaf_font_size=10,
        orientation = 'right',
        color_threshold = 4,
        ax = ax
)
plt.title('Dendrogramme de clustering hiérarchique')
plt.ylabel('Départements')
plt.xlabel('Distance (dissimilarité)')
plt.show()

#
#  FOCUS SUR LE DEPARTEMENT DE LA HAUTE-GARONNE
#
# Filtrer les lignes où le nom_département est 'Haute-Garonne'

# HG = salaire_geo.loc[salaire_geo['nom_département'] == 'Haute-Garonne']

# from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
# from sklearn.cluster import KMeans
# from sklearn.manifold import TSNE
# from sklearn.pipeline import make_pipeline
# from sklearn.preprocessing import StandardScaler

# # Étape 1 : Calcule les salaires moyens des colonnes "générales" par nom de département
# departements = HG.groupby('nom_commune')[['salaire', 'salaire_cadre', 'salaire_cadre_moyen', 'salaire_employe', 'salaire_travailleur']].mean().reset_index()
# print('Taille du Dataframe departements : ',departements.shape)

# # -----------------------------------------------------------------------------------------
# # On passe de 5 136 lignes du dataframe Saliare à 100 lignes agrégées par nom de département
# # -------------------------------------------------------------------------------------------

# # Si on garde les 5 136 lignes cad on ne peut pas le group by
# # departements = salaire_geo



# # Étape 2 : Extraire les labels des départements
# departements_labels = departements['nom_commune'].tolist()
# # Étape 3 : Créer la figure pour la visualisation
# fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,18))
# # Étape 4 : Extraire les valeurs des salaires et on convertit le DataFrame en un tableau NumPy de valeurs.
# departements_salaire = departements.loc[:,['salaire','salaire_cadre','salaire_cadre_moyen','salaire_employe','salaire_travailleur']].values

# # Étape 5 : Calculer la liaison hiérarchique
# # Nous utilisons la méthode de Ward pour calculer les liaisons hiérarchiques
# # entre les observations dans departements_salaire.
# # La méthode de Ward minimise la variance totale au sein de chaque cluster.
# mergings = linkage(departements_salaire, method='ward')

# # Étape 6 : Tracer le dendrogramme
# dend = dendrogram(mergings,
#         labels=departements_labels,
#         leaf_rotation=0,
#         leaf_font_size=10,
#         orientation = 'right',
#         color_threshold = 4,
#         ax = ax
# )
# plt.title('Dendrogramme de clustering hiérarchique')
# plt.ylabel('Départements')
# plt.xlabel('Distance (dissimilarité)')
# plt.show()

# On réduit les dimensions des données departements_salaire en utilisant le t-SNE,
# pour faciliter leur visualisation dans un espace en 2D.
# Cette technique permet ainsi l'interprétation et la présentation des résultats (Clusters).

tsne = TSNE(learning_rate=150)
departements_transformed = tsne.fit_transform(departements_salaire)
print("Taille du tableau numpy departements_transformed",departements_transformed.shape)

print(isinstance(departements_transformed, np.ndarray))

# Déterminons le nombre optimal de clusters avec la méthode du coude
inertia = []
K = range(1, 11)
for k in K:
    model = KMeans(n_clusters=k)
    model.fit(departements_transformed)
    inertia.append(model.inertia_)

# Tracer la courbe du coude
plt.figure(figsize=(8, 6))
plt.plot(K, inertia, 'bo-')
plt.xlabel('Nombre de clusters (k)')
plt.ylabel('Inertie')
plt.title('Méthode du coude pour déterminer le nombre optimal de clusters')
plt.show()

from sklearn.metrics import silhouette_score

# Calcul de l'indice de silhouette pour différents nombres de clusters
silhouette_scores = []
for k in range(2, 11):
    model = KMeans(n_clusters=k)
    labels = model.fit_predict(departements_transformed)
    score = silhouette_score(departements_transformed, labels)
    silhouette_scores.append(score)

# Tracer les scores de silhouette
plt.figure(figsize=(8, 6))
plt.plot(range(2, 11), silhouette_scores, 'bo-')
plt.xlabel('Nombre de clusters (k)')
plt.ylabel('Score de silhouette')
plt.title('Score de silhouette pour différents nombres de clusters')
plt.show()

from sklearn.metrics import calinski_harabasz_score

# Calcul de l'indice de Calinski-Harabasz pour différents nombres de clusters
calinski_harabasz_scores = []
for k in range(2, 11):
    model = KMeans(n_clusters=k)
    labels = model.fit_predict(departements_transformed)
    score = calinski_harabasz_score(departements_transformed, labels)
    calinski_harabasz_scores.append(score)

# Tracer les scores de Calinski-Harabasz
plt.figure(figsize=(8, 6))
plt.plot(range(2, 11), calinski_harabasz_scores, 'bo-')
plt.xlabel('Nombre de clusters (k)')
plt.ylabel('Indice de Calinski-Harabasz')
plt.title('Indice de Calinski-Harabasz pour différents nombres de clusters')
plt.show()

from sklearn.metrics import davies_bouldin_score

# Calcul de l'indice de Davies-Bouldin pour différents nombres de clusters
davies_bouldin_scores = []
for k in range(2, 11):
    model = KMeans(n_clusters=k)
    labels = model.fit_predict(departements_transformed)
    score = davies_bouldin_score(departements_transformed, labels)
    davies_bouldin_scores.append(score)

# Tracer les scores de Davies-Bouldin
plt.figure(figsize=(8, 6))
plt.plot(range(2, 11), davies_bouldin_scores, 'bo-')
plt.xlabel('Nombre de clusters (k)')
plt.ylabel('Indice de Davies-Bouldin')
plt.title('Indice de Davies-Bouldin pour différents nombres de clusters')
plt.show()

import pylab
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from scipy.spatial.distance import cdist

# Transformation par t-SNE
tsne = TSNE(learning_rate=150)
departements_transformed = tsne.fit_transform(departements_salaire)

# Saisir le nombre de clusters optimal en fonction des résultats obtenus lors des précédentes étapes
# nclust = int(input("Entrez le nombre optimal de clusters (k) selon les méthodes vues précédemment : "))
nclust=6
# Étape 1 : Création du modèle KMeans
model = KMeans(n_clusters=nclust)

# Étape 2 : Ajustement du modèle et obtention des étiquettes de cluster
model.fit(departements_transformed)
classes = model.labels_
centroids = model.cluster_centers_

# Étape 3 : Calcul des distances aux centroides respectifs
distances_to_own_centroid = cdist(departements_transformed, centroids, 'euclidean')
own_centroid_distances = np.array([distances_to_own_centroid[i, label] for i, label in enumerate(classes)])

# Distance euclidienne moyenne
distance_euclidienne_moyenne = np.mean(own_centroid_distances)
print(f"Distance euclidienne moyenne aux centroides : {distance_euclidienne_moyenne:.4f}")

# Silhouette score
silhouette_avg = silhouette_score(departements_transformed, classes)
print(f"Silhouette Score : {silhouette_avg:.4f}")



# Définir le nombre de couleurs pour la visualisation : Le nombre de couleurs est égal au nombre de clusters.
num_colors = nclust

# Obtenir la colormap avec 20 couleurs distinctes à partir de la bibliothèque pylab
cm = pylab.get_cmap('tab20')

# Création de la visualisation
fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 8))

# Tracer les clusters
for i in np.unique(classes):
    ix = np.where(classes == i)
    ax.scatter(departements_transformed[ix, 0], departements_transformed[ix, 1],
               c=cm(1 * i / num_colors), label=i)
    centroid_x = np.mean(departements_transformed[ix, 0])
    centroid_y = np.mean(departements_transformed[ix, 1])
    ax.scatter(centroid_x, centroid_y, c=cm(1 * i / num_colors), marker='x', s=400)
    ax.annotate(i, (centroid_x, centroid_y), c=cm(1 * i / num_colors),  fontsize=14,fontweight='bold')

# Ajouter des annotations
for label, x, y in zip(departements_labels, departements_transformed[:, 0], departements_transformed[:, 1]):
    # if label in ["Oise", "Haute-Loire","Vienne"]:
    #     plt.annotate(
    #         label,
    #         xy=(x + x / 100, y + y / 100),
    #         fontsize=12,
    #         fontweight='bold',
    #         color='red',
    #         alpha=0.8)
    # else:
        plt.annotate(
            label,
            xy=(x + x / 100, y + y / 100),
            fontsize=10,
            alpha=0.6)

plt.xlabel('Dimension réduite 1 (t-SNE)')
plt.ylabel('Dimension réduite 2 (t-SNE)')
plt.legend(loc="lower right")

# Assigner les clusters et les distances à son centroid à chaque département
departements['cluster'] = classes
departements['distance_son_centroid'] = own_centroid_distances


# Trier par cluster et distance à son centroid
departements_sorted = departements.sort_values(by=['cluster', 'distance_son_centroid'])


# Obtenir la première ligne de chaque cluster pour afficher le nom du département le plus proche de son centroid
departement_proche_centroid = departements_sorted.groupby('cluster').first()[['nom_département']]

# Département le plus proche de son centroid
print("Département le plus proche de son centroid par cluster :")
print(departement_proche_centroid)

# Obtenir la dernière ligne de chaque cluster pour afficher le nom du département le plus éloigné de son centroid
departement_eloigne_centroid = departements_sorted.groupby('cluster').last()[['nom_département']]

# Département le plus éloigné de son centroid
print("Département le plus éloigné de son centroid par cluster:")
print(departement_eloigne_centroid)



plt.show()

# Itérations sur chaque cluster unique.
for i in departements_sorted['cluster'].sort_values().unique():
    print("---------------------")
    print("Cluster " + str(i))
# Impressions des noms des départements qui appartiennent au cluster actuel.
    print(departements_sorted[departements['cluster'] == i][['nom_département','distance_son_centroid']])


# # Sélectionner les lignes où le nom du département est "Oise"
# oise_values = departements.loc[departements['nom_département'] == 'Oise']

# # Afficher les valeurs du département "Oise"
# print(oise_values)